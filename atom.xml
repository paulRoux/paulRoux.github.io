<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vkey Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://roux.top/"/>
  <updated>2018-08-31T12:35:03.158Z</updated>
  <id>http://roux.top/</id>
  
  <author>
    <name>Vkey</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>实习总结</title>
    <link href="http://roux.top/2018/08/31/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>http://roux.top/2018/08/31/实习总结/</id>
    <published>2018-08-30T16:00:00.000Z</published>
    <updated>2018-08-31T12:35:03.158Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;work hard， work smart！<br><a id="more"></a></p><p>&emsp;&emsp;不知不觉三个月的实习就结束了，也许只有到最后要走的时候才会真正的感受到一些别样的情绪充斥在心间。感谢这段时间师兄师姐们的指导与帮助，也很开心能够遇见这么多的小伙伴！<br>&emsp;&emsp;最终得知转正的结果还是很开心的，后面呢，也准备留在阿里了。以这段时间的感受来说：阿里是一家很有责任感的公司、公司内部的腐败程度个人感觉还是较其他公司好的、阿里的价值观我表示认同、最主要的是身边的小伙伴们都很nice。感觉这里面的环境很适合我，个人也比较懒，也就不想再换公司了（换公司的话，这种环境很难说）<br>&emsp;&emsp;实习期间学到的东西还是很多的，主要是开了眼界，思想上有了相应的提升，对大公司的流程什么的都有了了解，而且对自己后面的方向也能够做出一些选择。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;work hard， work smart！&lt;br&gt;
    
    </summary>
    
      <category term="生活" scheme="http://roux.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="随笔" scheme="http://roux.top/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Vmware的一些问题</title>
    <link href="http://roux.top/2018/05/10/Vmware%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"/>
    <id>http://roux.top/2018/05/10/Vmware的一些问题/</id>
    <published>2018-05-09T16:00:00.000Z</published>
    <updated>2018-05-18T08:40:15.531Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>前几天换了个电脑，于是所有的东西都需要拷贝和重装，在装vmware的时候遇到了一些问题，所以记录在这里。</li></ul><a id="more"></a><h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><ul><li><p>windows7</p><ul><li>可以安装任何的版本，而且安装所有的系统都是没问题的。要注意的就是对vmware的服务要保持开启，给centos7安装vmtools的时候有点麻烦。这里注意的没有什么太多。</li></ul></li><li><p>windows10</p><ul><li><p>这里的话在安装vmware的时候需要安装vmware12及以上的版本，官方给的说明是11的时候就开始支持windows10了，但是我在装ubuntu16.04的时候直接报错导致安装失败，之后我使用12版本的直接成功。在这里需要注意的是在装系统的时候需要把硬件定制里面的显示的3D图形加速关闭，否则会出现黑屏的现象。如果ubuntu出现屏幕分辨率的问题导致无法点击安装过程中的确认或者下一步按钮的情况，可以先试用，然后选择设置修改分辨率，让其适合当前的分辨率，然后再次进入就可以看到和选择了。</p></li><li><p>由于中间有一次直接用杀毒软件结束掉了VMware，导致我在下一次打开ubuntu的时候显示正在使用中，用了其他的一些方法也都没解决，最终找到一个方法简单粗暴：找到安装操作系统的目录，进去后会看到两个后缀为<code>.lck</code>的文件夹，把这两个文件夹剪切出来（删除掉也可以，你打开以后会重建），然后再打开虚拟机就没问题了。具体的原因还没去了解。</p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;前几天换了个电脑，于是所有的东西都需要拷贝和重装，在装vmware的时候遇到了一些问题，所以记录在这里。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Tools" scheme="http://roux.top/categories/Tools/"/>
    
    
      <category term="VMware" scheme="http://roux.top/tags/VMware/"/>
    
  </entry>
  
  <entry>
    <title>HTTP2.0</title>
    <link href="http://roux.top/2018/04/22/HTTP2.0/"/>
    <id>http://roux.top/2018/04/22/HTTP2.0/</id>
    <published>2018-04-21T16:00:00.000Z</published>
    <updated>2018-10-21T13:47:33.398Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li><p>这篇文章主要是对HTTP2.0进行一个总结，把零散的知识点进行连接。</p></li><li><p>HTTP2.0 的目的是通过支持请求与响应的多路复用来较少延迟，通过压缩 HTTPS 首部字段将协议开销降低，同时增加请求优先级和服务器端推送的支持.</p></li></ul><a id="more"></a><h3 id="二进制分帧层"><a href="#二进制分帧层" class="headerlink" title="二进制分帧层"></a>二进制分帧层</h3><ul><li><p>二进制分帧层，是HTTP 2.0性能增强的核心。 </p><ul><li>HTTP 1.x在应用层以纯文本的形式进行通信，而HTTP 2.0将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。这样，客户端和服务端都需要引入新的二进制编码和解码的机制。</li></ul></li><li><p><strong>帧</strong></p><ol><li><p><strong>帧的分类</strong></p><ol><li>DATA：用于传输 HTTP 消息体；</li><li>HEADERS：用于传输首部字段；</li><li>SETTINGS：用于约定客户端和服务端的配置数据。比如设置初识的双向流量控制窗口大小；</li><li>WINDOW_UPDATE：用于调整个别流或个别连接的流量</li><li>PRIORITY： 用于指定或重新指定引用资源的优先级。</li><li>RST_STREAM： 用于通知流的非正常终止。</li><li>PUSH_ PROMISE： 服务端推送许可。</li><li>PING： 用于计算往返时间，执行 “活性” 检活。</li><li>GOAWAY： 用于通知对端停止在当前连接中创建流<br><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/HTTP2.0.png" alt=""></li></ol><ul><li>标志位用于不同的帧类型定义特定的消息标志。比如 DATA 帧就可以使用End Stream: true表示该条消息通信完毕。流标识位表示帧所属的流 ID。优先值用于 HEADERS 帧，表示请求优先级。R 表示保留位。</li></ul></li><li><p><strong>消息</strong></p><ul><li>消息是指逻辑上的HTTP消息（请求/响应）。一系列数据帧组成了一个完整的消息。比如一系列DATA帧和一个HEADERS帧组成了请求消息。</li></ul></li><li><p><strong>流</strong></p><ol><li>流是连接中的一个虚拟信道，可以承载双向消息传输。每个流有唯一整数标识符。为了防止两端流 ID 冲突，客户端发起的流具有奇数 ID，服务器端发起的流具有偶数 ID。 </li><li>所有 HTTP2.0通信都在一个 TCP 连接上完成，慢启动时间减少,拥塞和丢包恢复速度更快，这个连接可以承载任意数量的双向数据流 Stream。相应地，每个数据流以 消息的形式发送，而消息由一 或多个帧组成，这些帧可以乱序发送，然后根据每个帧首部的流标识符重新组装。</li></ol></li></ol></li><li><p><strong><a href="https://www.nihaoshijie.com.cn/index.php/archives/698/" target="_blank" rel="noopener">多路复用</a></strong></p><ol><li><p>HTTP1.X如果不用<a href="https://blog.csdn.net/qq_28885149/article/details/52922107" target="_blank" rel="noopener">pipeling</a>的话会实现串行传输（一个得到应答才继续下一个），如果使用<a href="https://blog.csdn.net/qq_28885149/article/details/52922107" target="_blank" rel="noopener">pipeling</a>的话会<a href="https://blog.csdn.net/jiyiqinlovexx/article/details/50500246" target="_blank" rel="noopener">建立多条TCP连接</a>，这会导致开销增大</p></li><li><p>HTTP2.0建立一条TCP连接后，会并行传输数据。HTTP 2.0成功解决了HTTP 1.x的队首阻塞问题（TCP层的阻塞仍无法解决），同时，也不需要通过pipeline机制多条TCP连接来实现并行请求与响应。减少了TCP连接数对服务器性能也有很大的提升。</p></li><li><p>HTTP1.X虽然可以采用keep alive来解决复用TCP的问题，但是还是无法解决请求阻塞问题。</p><ul><li>所谓请求阻塞意思就是一条TCP的connection在同一时间只能允许一个请求经过，这样假如后续请求想要复用这个链接就必须等到前一个完成才行，正如上图左边表示的。</li><li>之所以有这个问题就是因为HTTP1.x需要每条请求都是可是识别，按顺序发送，否则server就无法判断该相应哪个具体的请求。</li><li>HTTP2采用多路复用是指，在同一个域名下，开启一个TCP的connection，每个请求以stream的方式传输，每个stream有唯一标识，connection一旦建立，后续的请求都可以复用这个connection并且可以同时发送，server端可以根据stream的唯一标识来相应对应的请求。</li></ul></li></ol></li><li><p><strong>请求优先级</strong></p><ul><li><p>流可以带有一个31bit的优先级：</p><ul><li>0：表示最高优先级</li><li>2^31-1：表示最低优先级</li></ul></li><li><p>客户端明确指定优先级，服务端可以根据这个优先级作为依据交互数据，比如客户端优先级设置为.css&gt;.js&gt;.jpg（具体可参见《高性能网站建设指南》）， 服务端按优先级返回结果有利于高效利用底层连接，提高用户体验。 </p></li><li><p>然而，也不能过分迷信请求优先级，仍然要注意以下问题：</p><ul><li>服务端是否支持请求优先级</li><li>会否引起队首阻塞问题，比如高优先级的慢响应请求会阻塞其他资源的交互。</li></ul></li></ul></li><li><p><strong>服务端推送</strong></p><ul><li>HTTP 2.0增加了服务端推送功能，服务端可以根据客户端的请求，提前返回多个响应，推送额外的资源给客户端。<ul><li>比如：客户端请求stream 1，/page.html。服务端在返回stream 1消息的同时推送了stream 2（/script.js）和stream 4（/style.css）。</li></ul></li></ul></li><li><p><strong>首部压缩</strong></p><ol><li>HTTP1.x在每一次通信都会携带首部信息描述资源属性</li><li>HTTP2.0会在双方之间使用“首部表”来跟踪之前发送和存储的键值对，首部表在链接过程中始终存在，新增的键值对会更新到表尾，因此不需要每次通信都携带首部</li><li>HTTP2.0用到了首部压缩技术，压缩算法为HPACK<ul><li>HTTP 2.0关注的是首部压缩，而我们常用的gzip等是报文内容（body）的压缩。二者不仅不冲突，且能够一起达到更好的压缩效果。</li></ul></li></ol></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://blog.csdn.net/zhuyiquan/article/details/69257126" target="_blank" rel="noopener">https://blog.csdn.net/zhuyiquan/article/details/69257126</a></li><li><a href="https://segmentfault.com/a/1190000013028798" target="_blank" rel="noopener">https://segmentfault.com/a/1190000013028798</a></li><li><a href="https://www.nihaoshijie.com.cn/index.php/archives/698/" target="_blank" rel="noopener">https://www.nihaoshijie.com.cn/index.php/archives/698/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;这篇文章主要是对HTTP2.0进行一个总结，把零散的知识点进行连接。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;HTTP2.0 的目的是通过支持请求与响应的多路复用来较少延迟，通过压缩 HTTPS 首部字段将协议开销降低，同时增加请求优先级和服务器端推送的支持.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>DNS解析的过程</title>
    <link href="http://roux.top/2018/04/22/DNS%E8%A7%A3%E6%9E%90/"/>
    <id>http://roux.top/2018/04/22/DNS解析/</id>
    <published>2018-04-21T16:00:00.000Z</published>
    <updated>2018-04-22T06:26:43.626Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>对于网络这一块，有太多的学问，而且这个也是后面重点需要优化的方向，所以最近会写一些这方面的文章。</li></ul><a id="more"></a><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ol><li>在浏览器中输入<a href="http://www.qq.com域名，操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。" target="_blank" rel="noopener">www.qq.com域名，操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。</a> </li><li>如果hosts里没有这个域名的映射，则查找本地DNS解析器缓存，是否有这个网址映射关系，如果有，直接返回，完成域名解析。 </li><li>如果hosts与本地DNS解析器缓存都没有相应的网址映射关系，首先会找TCP/ip参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，此服务器收到查询时，如果要查询的域名，包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析，此解析具有权威性。 </li><li>如果要查询的域名，不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。 </li><li>如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置（是否设置转发器）进行查询，如果未用转发模式，本地DNS就把请求发至13台根DNS，根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，它就会找一个管理.com域的下一级DNS服务器地址(qq.com)给本地DNS服务器。当本地DNS服务器收到这个地址后，就会找qq.com域服务器，重复上面的动作，进行查询，直至找到<a href="http://www.qq.com主机。" target="_blank" rel="noopener">www.qq.com主机。</a> </li><li>如果用的是转发模式，此DNS服务器就会把请求转发至上一级DNS服务器，由上一级服务器进行解析，上一级服务器如果不能解析，或找根DNS或把转请求转至上上级，以此循环。不管是本地DNS服务器用是是转发，还是根提示，最后都是把结果返回给本地DNS服务器，由此DNS服务器再返回给客户机。 </li></ol><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ul><li><p>人们在 DNS 中发现了一些漏洞，攻击者可以利用这些漏洞劫持这一使用名称在 互联网 上搜寻某个人或某个站点的过程。这种攻击的目的是取得对会话的控制以实施某种操作，例如使用户进入劫持者自己设立的欺骗性网站，以便收集用户的帐户和密码。</p></li><li><p>安全的DNS—<a href="https://www.icann.org/resources/pages/dnssec-qaa-2014-01-29-zh" target="_blank" rel="noopener">DNSSEC</a></p></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://www.nowcoder.com/profile/2286733/myFollowings/detail/4794271" target="_blank" rel="noopener">https://www.nowcoder.com/profile/2286733/myFollowings/detail/4794271</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对于网络这一块，有太多的学问，而且这个也是后面重点需要优化的方向，所以最近会写一些这方面的文章。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="DNS" scheme="http://roux.top/tags/DNS/"/>
    
  </entry>
  
  <entry>
    <title>HTTP与HTTPS</title>
    <link href="http://roux.top/2018/04/22/HTTP%E4%B8%8EHTTPS/"/>
    <id>http://roux.top/2018/04/22/HTTP与HTTPS/</id>
    <published>2018-04-21T16:00:00.000Z</published>
    <updated>2018-04-22T06:28:42.803Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>HTTPS是对HTTP在安全方面上做的一个改进，HTTPS = HTTP + <a href="https://zh.wikipedia.org/wiki/%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A" target="_blank" rel="noopener">SSL/TLS</a></li></ul><a id="more"></a><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ol><li><p>https需要申请CA证书，需要一定的费用</p></li><li><p>http是明文传输，https是有安全性的ssl加密传输</p></li><li><p>http的端口是80，https是443</p></li><li><p>http是简单的，<a href="http://www.cnblogs.com/bellkosmos/p/5237146.html" target="_blank" rel="noopener">无状态</a>的，https是由http+sslL组成的进行加密传输、身份认证的协议，更加安全</p></li></ol><h3 id="HTTPS过程"><a href="#HTTPS过程" class="headerlink" title="HTTPS过程"></a>HTTPS过程</h3><ol><li><p>客户端发起请求，将自己支持的加密算法，打个包告诉服务器端</p></li><li><p>服务器端从客户端发来的加密算法中，选出一组加密算法和HASH算法（注，HASH也属于加密），并将自己的身份信息以证书的形式发回给客户端。而证书中包含了网站的地址，加密用的公钥，以及证书的颁发机构等；这里，服务器就将自己用来加密用的公钥一同发还给客户端，而私钥则服务器保存着，用户解密客户端加密过后的内容。</p></li><li><p>客户端收到内容</p><ol><li>验证一下证书是否合法。一般来说，证书是用来标示一个站点是否合法的标志。如果说该证书由权威的第三方颁发和签名的，则说明证书合法。</li><li>如果证书合法，或者客户端接受和信任了不合法的证书，则客户端就会随机产生一串序列号，使用服务器发来的公钥进行加密。这时候，一条返回的消息就基本就绪。</li><li>最后使用服务器挑选的HASH算法，将刚才的消息使用刚才的随机数进行加密，生成相应的消息校验值，与刚才的消息一同发还给服务器。</li></ol></li><li><p>服务器接受到客户端发来的消息后</p><ol><li>使用私钥解密上面客户端公钥加密的消息，得到客户端产生的随机序列号</li><li>使用该随机序列号，对该消息进行加密，验证的到的校验值是否与客户端发来的一致。如果一致则说明消息未被篡改，可以信任</li><li>最后，使用该随机序列号，加上之前第2步中选择的加密算法，加密一段握手消息，发还给客户端。同时HASH值也带上</li></ol></li><li><p>客户端收到服务器端的消息后</p><ol><li>计算HASH值是否与发回的消息一致</li><li>检查消息是否为握手消息</li></ol></li><li><p>握手结束后，客户端和服务器端使用握手阶段产生的随机数以及挑选出来的算法进行对称加解密的传输</p></li></ol><h3 id="为什么不直接全程使用非对称加密算法进行数据传输"><a href="#为什么不直接全程使用非对称加密算法进行数据传输" class="headerlink" title="为什么不直接全程使用非对称加密算法进行数据传输"></a>为什么不直接全程使用非对称加密算法进行数据传输</h3><ol><li><p>因为非对称算法的效率对比起对称算法来说，要低得多得多；因此往往只用在HTTPS的握手阶段。</p></li><li><p>http建立连接114ms，https为436ms（ssl为322ms），针对computer science house（计算机科学院CSH）的测试</p></li></ol><h3 id="经常使用的加密算法"><a href="#经常使用的加密算法" class="headerlink" title="经常使用的加密算法"></a>经常使用的加密算法</h3><ol><li>非对称加密算法：RSA, DSA/DSS</li><li>对称加密算法： AES, 3DES</li><li>HASH算法：MD5, SHA1, SHA256</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;HTTPS是对HTTP在安全方面上做的一个改进，HTTPS = HTTP + &lt;a href=&quot;https://zh.wikipedia.org/wiki/%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSL/TLS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>HTTP学习（六）——HTTP2性能优化实践</title>
    <link href="http://roux.top/2018/04/22/HTTP%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://roux.top/2018/04/22/HTTP学习（六）/</id>
    <published>2018-04-21T16:00:00.000Z</published>
    <updated>2018-04-22T06:06:02.928Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>为了让我们的网站访问速度快变，我们不断增加带宽，做针对HTTP协议的特点对应用做各种优化。希望能通常高带宽，低延迟来提高网站到访问速度。带宽与延迟两者间，延迟才是性能的真正瓶颈。</li></ul><a id="more"></a><h3 id="客观原因"><a href="#客观原因" class="headerlink" title="客观原因"></a>客观原因</h3><ul><li><p>当我们针对HTTP协议进行性能优化时，我们不得不注意到这样的一些客观原因。通常当我们改变不了底层协议特性时，我们只能在更高层处做文章。</p><ol><li><p>带宽受物理层介质的影响，最快快不过光速，但是即便在今天越来越高的带宽，我们依然没有感觉到网站访问速度有多快。 </p></li><li><p>延迟包括传输延迟，传播延迟，处理延迟和排队延迟。</p></li><li><p>每次连接都需要经过3次握手才能建立，增加整整一个RTT。</p></li><li><p>TCP拥塞控制策略：慢启动，拥塞避免[注1]。其中慢启动会被应用到每一个连接中。TCP流量和拥塞控制会影响整个网络到吞吐量。</p></li><li><p>解析DNS必然会产生一些延迟。</p></li><li><p>最快获得资源的方式，莫过于还没传，就已拿到。充分利用缓存。</p></li><li><p>最少的延迟就是什么都不传。而没有延迟。</p></li></ol></li></ul><h3 id="针对HTTP1-X做过的优化"><a href="#针对HTTP1-X做过的优化" class="headerlink" title="针对HTTP1.X做过的优化"></a>针对HTTP1.X做过的优化</h3><ul><li><p>由于TCP层的特性，在提高web性能时，我们不得不做一些优化， <a href="https://developer.yahoo.com/performance/rules.html" target="_blank" rel="noopener">Best Practices for Speeding Up Your Web Site</a>，这些优化中针对HTTP协议特点的，我们做了哪些呢，从总体来说有以下几大点。</p><ol><li><p>连接和拼接</p><ul><li>连接或拼接JS和CSS文件，雪碧图，以减少HTTP请求，同时浏览器可缓存这些静态资源，为下次访问节约时间。但是这样带来的副作用是，维护成本高，其中某一个小改动都会使得整个拼接后的文件发生改变，重新缓存。</li></ul></li><li><p>域名分区</p><ul><li>由于浏览器的限制，同一个域下最多只能建立6个连接。我们通常使用子域名来减少所有资源在只有一个连接时的产生的排队延迟。这个显然不适用在HTTP2中，因为不同的域需要建立不同的连接。</li></ul></li><li><p>资源内嵌</p><ul><li>对于不常用的，较小大资源内嵌在文档中，比如base64的图片，以减少HTTP请求，但是这样的资源不能在浏览器中缓存，也不可能被其他页面共享，同时还有可能编码之后的资源变等更大了。在HTTP2中，这样的资源就可以使用SERVER PUSH来推送。</li></ul></li><li><p>HTTP管道（额外的）</p><ul><li><p>在HTTP1.x中已经实现了持久连接，但是却依然需要排队来发送和接收请求，这样不能充分利用网络资源。因此提出了HTTP管道的概念。客户端可以将所有请求都一起发给服务器端，服务器端或者按顺序处理，或者并行处理这些请求。但是返回响应的顺序是按照其自己内部的优先级来的。</p></li><li><p>但是，其缺点也很多，以至于现在的浏览器对它还是默认关闭的：</p></li></ul></li><li><p>队首阻塞</p><ul><li>并行处理时，需要占用缓存资源，服务器容易受到攻击</li><li>响应失败后，会断开tcp连接，并要求重发之后的所有请求，造成资源浪费</li><li>中间代理对其兼容性不是很好，有可能还会串行所有请求</li></ul></li></ol></li></ul><h3 id="针对HTTP2需要做的优化"><a href="#针对HTTP2需要做的优化" class="headerlink" title="针对HTTP2需要做的优化"></a>针对HTTP2需要做的优化</h3><ul><li><p>坚决去掉在HTTP1.X中的域名分区，连接和拼接和资源内嵌的优化方式。</p></li><li><p>尽量让所有资源在同一域名下</p></li><li><p>利用服务器推送</p></li><li><p>继续保留CDN</p></li></ul><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><ul><li>TCP慢开始与拥塞避免示意图<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/six/slow-start.png" alt=""></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://imjiaolong.cn/post/http2vshttp1.1.html" target="_blank" rel="noopener">https://imjiaolong.cn/post/http2vshttp1.1.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;为了让我们的网站访问速度快变，我们不断增加带宽，做针对HTTP协议的特点对应用做各种优化。希望能通常高带宽，低延迟来提高网站到访问速度。带宽与延迟两者间，延迟才是性能的真正瓶颈。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>HTTP1.0与HTTP1.1</title>
    <link href="http://roux.top/2018/04/22/HTTP1.0%E4%B8%8EHTTP1.1/"/>
    <id>http://roux.top/2018/04/22/HTTP1.0与HTTP1.1/</id>
    <published>2018-04-21T16:00:00.000Z</published>
    <updated>2018-04-22T12:26:10.026Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>前面的系列文章最终感觉有点零散，所以在这里将HTTP1.0和HTTP1.1的一些区别进行说明。</li></ul><a id="more"></a><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li><p><strong>可扩展性</strong></p><ol><li>为了与未来的协议规范兼容，HTTP1.1在请求消息中包含了Upgrade头域，客户端可以让服务器知道它能够支持的其他备用通信协议，服务器以此进行协议的切换</li><li>HTTP1.1增加了options方法，允许客户端获取服务器支持的方法列表</li><li>在消息中增加版本号，用来进行兼容判断。此版本号只可以判断逐段的兼容性，不能判断端到端的兼容性<ul><li>eg：一台HTTP/1.1的源服务器从使用HTTP/1.1的Proxy那儿接收到一条转发的消息，实际上源服务器并不知道终端客户使用的是HTTP/1.0还是HTTP/1.1。因此，HTTP/1.1定义Via头域，用来记录消息转发的路径，它记录了整个路径上所有发送方使用的版本号</li></ul></li></ol></li><li><p><strong>缓存</strong></p><ol><li>HTTP1.1在1.0的特性上加了一些cache的特性：当缓存的对象的age超过expire（最大时限）时变为stale（旧的）对象，cache不需要直接抛弃此对象，而是与服务器进行重新激活</li><li>HTTP1.1增加了Cache-Control头域（请求消息和响应消息都可用），它支持一个可扩展的指令子集：例如max-age指令支持相对时间戳；private和no-store指令禁止对象被缓存；no-transform阻止Proxy进行任何改变响应的行为</li></ol></li><li><p><strong>带宽优化</strong></p><ol><li>请求对象的部分<ul><li>HTTP/1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了。例如，客户端只需要显示一个文档的部分内容，又比如下载大文件时需要支持断点续传功能，而不是在发生断连后不得不重新下载完整的包。</li><li>HTTP/1.1中在请求消息中引入了range头域，它允许只请求资源的某个部分。在响应消息中Content-Range头域声明了返回的这部分对象的偏移值和长度。如果服务器相应地返回了对象所请求范围的内容，则响应码为206（Partial Content），它可以防止Cache将响应误以为是完整的一个对象</li></ul></li><li>请求消息中如果包含比较大的实体内容，但不确定服务器是否能够接收该请求（如是否有权限），此时若贸然发出带实体的请求，如果被拒绝也会浪费带宽<ul><li>HTTP/1.1加入了一个新的状态码100（Continue）。客户端事先发送一个只带头域的请求，如果服务器因为权限拒绝了请求，就回送响应码401（Unauthorized）；如果服务器接收此请求就回送响应码100，客户端就可以继续发送带实体的完整请求了。注意，HTTP/1.0的客户端不支持100响应码。但可以让客户端在请求消息中加入Expect头域，并将它的值设置为100-continue</li></ul></li><li>节省带宽资源的一个非常有效的做法就是压缩要传送的数据。Content-Encoding是对消息进行端到端（end-to-end）的编码，它可能是资源在服务器上保存的固有格式（如jpeg图片格式）；在请求消息中加入Accept-Encoding头域，它可以告诉服务器客户端能够解码的编码方式</li></ol></li><li><p><strong>长连接</strong></p><ol><li>HTTP 1.0规定浏览器与服务器只保持短暂的连接，浏览器的每次请求都需要与服务器建立一个TCP连接，服务器完成请求处理后立即断开TCP连接，服务器不跟踪每个客户也不记录过去的请求。此外，由于大多数网页的流量都比较小，一次TCP连接很少能通过slow-start区，不利于提高带宽利用率。<ul><li>在HTTP/1.0中，要建立长连接，可以在请求消息中包含Connection: Keep-Alive头域，如果服务器愿意维持这条连接，在响应消息中也会包含一个Connection: Keep-Alive的头域。同时，可以加入一些指令描述该长连接的属性，如max，timeout等</li><li>通常，HTTP/1.0的Proxy不支持Connection头域，为了不让它们转发可能误导接收者的头域，协议规定所有出现在Connection头域中的头域名都将被忽略</li></ul></li><li>HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。例如：一个包含有许多图像的网页文件的多个请求和应答可以在一个连接中传输，但每个单独的网页文件的请求和应答仍然需要使用各自的连接。</li><li>HTTP 1.1还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容，这样也显著地减少了整个下载过程所需要的时间</li></ol></li><li><p><strong>消息传递</strong></p><ol><li>HTTP消息中可以包含任意长度的实体，通常它们使用Content-Length来给出消息结束标志。但是，对于很多动态产生的响应，只能通过缓冲完整的消息来判断消息的大小，但这样做会加大延迟。如果不使用长连接，还可以通过连接关闭的信号来判定一个消息的结束。<ol><li>HTTP/1.1中引入了Chunkedtransfer-coding来解决上面这个问题，发送方将消息分割成若干个任意大小的数据块，每个数据块在发送时都会附上块的长度，最后用一个零长度的块作为消息结束的标志。这种方法允许发送方只缓冲消息的一个片段，避免缓冲整个消息带来的过载。</li><li>在HTTP/1.0中，有一个Content-MD5的头域，要计算这个头域需要发送方缓冲完整个消息后才能进行。而HTTP/1.1中，采用chunked分块传递的消息在最后一个块（零长度）结束之后会再传递一个拖尾（trailer），它包含一个或多个头域，这些头域是发送方在传递完所有块之后再计算出值的。发送方会在消息中包含一个Trailer头域告诉接收方这个拖尾的存在。</li></ol></li></ol></li><li><p><strong>Host头域</strong></p><ol><li>在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。</li><li>HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。此外，服务器应该接受以绝对路径标记的资源请求。</li></ol></li><li><p><strong>内容协商</strong></p><ol><li>为了满足互联网使用不同母语和字符集的用户，一些网络资源有不同的语言版本（如中文版、英文版）。HTTP/1.0定义了内容协商（contentnegotiation）的概念，也就是说客户端可以告诉服务器自己可以接收以何种语言（或字符集）表示的资源。<ol><li>例如如果服务器不能明确客户端需要何种类型的资源，会返回300（Multiple Choices），并包含一个列表，用来声明该资源的不同可用版本，然后客户端在请求消息中包含Accept-Language和Accept-Charset头域指定需要的版本。</li></ol></li></ol></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p>详解HTTP1.1：<a href="http://imweb.io/topic/554c5879718ba1240cc1dd8a" target="_blank" rel="noopener">http://imweb.io/topic/554c5879718ba1240cc1dd8a</a></p></li><li><p><a href="https://blog.csdn.net/forgotaboutgirl/article/details/6936982" target="_blank" rel="noopener">https://blog.csdn.net/forgotaboutgirl/article/details/6936982</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;前面的系列文章最终感觉有点零散，所以在这里将HTTP1.0和HTTP1.1的一些区别进行说明。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>HTTP学习（五）——HTTP2 VS SPDY</title>
    <link href="http://roux.top/2018/04/21/HTTP%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://roux.top/2018/04/21/HTTP学习（五）/</id>
    <published>2018-04-20T16:00:00.000Z</published>
    <updated>2018-04-22T05:45:56.796Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>SPDY是HTTP2的催化剂，但SPDY不是HTTP2。 本文主要针对SPDY与HTTP2之间的区别，而不太多的涉及它俩之间与HTTP1.X的区别。</li></ul><h3 id="SPDY的出现"><a href="#SPDY的出现" class="headerlink" title="SPDY的出现"></a>SPDY的出现</h3><ul><li><p>SPDY是谷歌在09年年中时发布的，它的主要目标时通过解决HTTP1.1中一些显著的性能限制，来减少网页额的加载时间。目前为止，Chrome,Firefox和Opera都支持了这一协议。</p></li><li><p>SPDY引入了新的二进制分帧层，以实现多路复用、优先次序、最小化的消除网络延迟，同时对HTTP首部进行压缩，减少HTTP报文的冗余数据。</p><blockquote><p>目前为止，我们只在实验室条件下测试过 SPDY。最初的成果很激动人心：通过模拟的家庭上网线路下载了 25 个最流行的网站之后，我们发现性能的改进特别明显，页面加载速度最多快了 55%。 ——A 2x Faster Web Chromium Blog</p></blockquote></li></ul><a id="more"></a><ul><li><p>一方面由于随着web应用的发展，HTTP1.1协议的局限性突显的越来越严重，另一方面由于SPDY的优秀表现，12年初，W3C向全社会征集HTTP2的建议，最终决定将SPDY规范作为制定标准的基础。随后的时间内，SPDY与HTTP2共同进化，HTTP2提出新规范或新功能，SPDY为它进行测试和验证。当HTTP2一切就绪之日，就是SPDY退出舞台之时。事实上，在今年2月谷歌公司已经宣布将在16年年初放弃对SPDY的支持。</p></li><li><p>目前各浏览器对SPDY和HTTP2的支持情况分别如下:<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/spdy-caniuse.png" alt=""><br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/http2-caniuse.png" alt=""></p><h3 id="SPDY与HTTP2的区别"><a href="#SPDY与HTTP2的区别" class="headerlink" title="SPDY与HTTP2的区别"></a>SPDY与HTTP2的区别</h3></li><li><p>头部压缩算法，SPDY，通用的deflate算法[注1]；HTTP2，专门为压缩头部设计的HPACK算法</p></li><li>SPDY必须在TLS上运行，HTTP2可在TCP上直接使用，因为增加了HTTP1.1的Upgrade机制</li><li>更加完善的协议商讨和确认流程</li><li>更加完善的Server Push流程</li><li>增加控制帧的种类，并对帧的格式考虑的更细致</li></ul><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><ul><li><p>deflate算法 这个算法是由2个算法组合而成，哈夫曼编码和LZ77编码。</p><ul><li><p>哈夫曼编码 一种无前缀编码，简单来说就是长字符串用短编码表示，以最终达到减少总大小的目的。具体编码过程可参考Huffman 编码压缩算法</p></li><li><p>例如，在一篇英语文章中，字母“E”出现的频率最高，“Z”最低，如果我们采用字符编码，那么每一个字母都是8bit表示；但是如果，我们使用不定长的bit编码，频率高的字母用比较短的编码表示，频率低的字母用长的编码表示，就会大大缩小文件的空间。</p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">字符编码</span><br><span class="line">A00101001</span><br><span class="line">B00101010</span><br><span class="line">C00101011……</span><br><span class="line">哈夫曼编码</span><br><span class="line">A    <span class="number">0</span></span><br><span class="line">B    <span class="number">10</span></span><br><span class="line">C    <span class="number">110</span></span><br><span class="line">D    <span class="number">1110</span></span><br><span class="line">E    <span class="number">11110</span></span><br></pre></td></tr></table></figure><p>大致思路：每次总是选取频率最小两个节点，将其频率相加，最终构成一个最优二叉树。 例如：有A,B,C,D,E五个字符，出现的频率（即权值）分别为5,4,3,2,1,那么我们第一步先取两个最小权值作为左右子树构造一个新树，即取1，2构成新树，其结点为1+2=3<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/hfm-1.png" alt=""><br>虚线为新生成的结点，第二步再把新生成的权值为3的结点放到剩下的集合中，所以集合变成{5,4,3,3}，再根据第二步，取最小的两个权值构成新树，如图：<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/hfm-2.png" alt=""><br>再依次建立哈夫曼树:<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/hfm-3.png" alt=""><br>其中各个权值替换对应的字符即为下图:<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/hfm-4.png" alt=""><br>所以各字符对应的编码为：A-&gt;11,B-&gt;10,C-&gt;00,D-&gt;011,E-&gt;010</p></li><li><p>LZ77编码 字典压缩算法,用到的场景比如gzip。</p><ul><li><p>这个算法的主要思想是：文件中有两块内容相同的话，那么只要知道前一块的位置和大小，我们就可以确定后一块的内容。所以我们可以用（两者之间的距离，相同内容的长度）这样一对信息，来替换后一块内容。由于（两者之间的距离，相同内容的长度）这一对信息的大小，小于被替换内容的大小，所以文件得到了压缩。</p></li><li><p>大致流程如下： 可构想出2个窗口，一个作为搜索缓存区（已完成搜索的字符），一个作为待搜索窗口。如下图:<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/five/lz77.jpg" alt=""></p></li><li>编码过程：<ul><li>如果待搜索字符c没在搜索缓存区中找到，则输出(0,0,c),同时，整个窗口向前移动1位</li><li>如果待搜索字符c在搜索缓存区中找到，起始位置在搜索缓存区中x，连续长度是offset，待搜索窗口中offset之后的一个字符是d，则输出(x, offset, d)。之后，整个窗口向前移动offset位。</li><li>最终输出的三元组就是压缩码</li></ul></li></ul></li></ul></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://imjiaolong.cn/post/http2%26spdy.html" target="_blank" rel="noopener">https://imjiaolong.cn/post/http2%26spdy.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;SPDY是HTTP2的催化剂，但SPDY不是HTTP2。 本文主要针对SPDY与HTTP2之间的区别，而不太多的涉及它俩之间与HTTP1.X的区别。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;SPDY的出现&quot;&gt;&lt;a href=&quot;#SPDY的出现&quot; class=&quot;headerlink&quot; title=&quot;SPDY的出现&quot;&gt;&lt;/a&gt;SPDY的出现&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SPDY是谷歌在09年年中时发布的，它的主要目标时通过解决HTTP1.1中一些显著的性能限制，来减少网页额的加载时间。目前为止，Chrome,Firefox和Opera都支持了这一协议。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SPDY引入了新的二进制分帧层，以实现多路复用、优先次序、最小化的消除网络延迟，同时对HTTP首部进行压缩，减少HTTP报文的冗余数据。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目前为止，我们只在实验室条件下测试过 SPDY。最初的成果很激动人心：通过模拟的家庭上网线路下载了 25 个最流行的网站之后，我们发现性能的改进特别明显，页面加载速度最多快了 55%。 ——A 2x Faster Web Chromium Blog&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
  </entry>
  
  <entry>
    <title>HTTP学习（四）——HTTP2.0的新特性</title>
    <link href="http://roux.top/2018/04/21/HTTP%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://roux.top/2018/04/21/HTTP学习（四）/</id>
    <published>2018-04-20T16:00:00.000Z</published>
    <updated>2018-04-22T05:46:02.851Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li><p>HTTP2.0可以让我们的应用变得更快、更简单、更健壮，让我们在HTTP/1.1时针对TCP协议特性而做的用来提高性能的HACK一笔勾销</p></li><li><p>为了提高应用的性能，降低延迟，我们能做的无外乎2点，要么传输的东西越小越好，要么距离能获得资源的地方越近越好。</p><ul><li>HTTP2的目的：通过支持<strong>多路复用</strong>来提高并行能力，减少因为3次握手等而产生的延迟；通过<strong>压缩HTTP首部</strong>将协议开销降到最低，同时支持请求优先级和服务器推送。</li><li>HTTP2最大的改变：引入<strong>二进制分帧层</strong>。HTTP2.0不会改动HTTP1.x的语义，提供的功能也不变，但是HTTP2对内修改了格式化数据的方式，以及传输这些数据的方式。对外，也就是面向应用，不用做任何改变，感知不到这一层的变化的。</li></ul></li><li><p>其实HTTP2.0是对HTTP1.x的一个扩展，而非替代，之所以称之为2是因为它引入的二进制分帧层之前的HTTP1.x的服务端和客户端并不兼容。</p><a id="more"></a></li></ul><h3 id="特性浅析"><a href="#特性浅析" class="headerlink" title="特性浅析"></a>特性浅析</h3><h4 id="分帧"><a href="#分帧" class="headerlink" title="分帧"></a><strong>分帧</strong></h4><ul><li><p>这是HTTP2.0中最大的改变。HTTP2.0之所以性能会比HTTP1.x有那么大的提高，很大程度上正是由于这一层的引入。</p><blockquote><p>这里所谓的“层” ，指的是位于套接字接口[1]与应用可见的高层HTTP API之间的一个新机制:HTTP的语义，包括各种动词、方法、首部，都不受影响，不同的是<strong>传输期间</strong>对它们的编码方式变了。</p></blockquote></li><li><p>HTTP1.x用回车换行符作为纯文本的分隔符，在进行解析和差错检测时不方便。HTTP1.x中用ASCII码，是16进制的，来表示报文中的每一个字符，如下图中,47代表字母G，45代表字符E，54代表字符T。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/http-header.png" alt=""></p></li><li><p>然而，HTTP2.0引入分帧层后，将报文分隔成一个个更小的帧，并采用二进制编码的方式。通常会将一个消息（首部和数据在一起的）分成一个HEADER帧和若干个DATA帧。如下图所示</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/frame-layer.png" alt=""></p><pre><code>另外需要明确的几个定义： **流**：已建立的连接上的双向字节流。具有唯一的流ID，客户端发起的为奇数ID，服务端发起的为偶数ID。很多个流可以并行的在同一个tcp连接上交换消息。 **消息**：与逻辑消息对应，比如一个请求或一个响应。由一个或多个帧组成。 **帧**：HTTP2中最小的通信单位，每个帧都会有帧首部，每个帧或者用来承载HTTP首部或负荷数据，或其他特定类型的帧。帧是遵循二进制编码的。总得来说，就是这样的，在HTTP2中，相同域名下的所有通信都在一个连接上完成，这个连接中可以承载任意数量的双向流。这些流都是以消息的形式被发送的，同时消息又由一个或多个帧组成。多个帧之间可以乱序发送，最后根据帧首部的流标识重新组装。</code></pre></li><li><p><strong>解释</strong> :这个双向是指，服务器可以给浏览器发消息（server push），浏览器也可以给服务器发东西（这就不用说啦）</p></li></ul><ul><li><p>对于一个帧来说，有固定固定帧格式，其中帧首部规定了帧最多只能带64KB的数据，还包括了帧类型和流标识符等。另外，帧中还可以填充一些额外的数据，最多255字节，保证数据安全性，拿HEADER帧举例。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/frame-structure.png" alt=""></p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/frame-header.png" alt=""></p></li><li><p>从这点上看，HTTP2.0中的帧与tcp报文段有些相似的。</p><p>在客户端或服务端发起建立新流时，帧携带HTTP的首部块，其中服务端发起流时，发送的HEADER帧没有优先级这一字段；当新流建立之后，就可发送HTTP消息的应用数据，也就是帧的负荷数据。</p><p>将消息拆成多个数据帧之后，会大大缓解HTTP队首阻塞[2]的情况。但是与tcp层的队首阻塞[3]并无直接关系。同时，改以帧为传输单位后，使得对报文无论是解析和差错检测方面都变得更加容易，因为对纯文本的解析还需要考虑到空格，空行，换行等问题。另外，也还消除了并行处理和发送请求及响应时对多个连接的依赖。</p></li><li><p><strong>解释</strong> ：多路复用：用一个tcp连接，并行发好多。【…keep_alive 很像啊】 —分组数？？表示是底层的，都没有概念…按道理说是分帧了后，分组数会变多啊？？？ 和keep_alive 不像，是和管道化很像哈。 实际上是这样的，首先肯定的是多使用同一个tcp连接，比起以前的多个tcp连接，会至少每次少了3个建立连接的tcp报文段，还不包括重传的。同时因为分帧之后出错的概率变小，间接的需要重传的包变少。从这两方面来说整个网络中的包中的总数是变少的。 另外呢，tcp报文段的长度其实还是保持不变的，不同流中的帧其实是混在一个tcp报文段中一起被发送，而在接收方那边接受到这个报文段后再进行拆分重新组装成新的http报文。</p></li></ul><h4 id="首部压缩"><a href="#首部压缩" class="headerlink" title="首部压缩"></a><strong>首部压缩</strong></h4><ul><li>由于HTTP协议是一种无状态的协议，服务器自身不会保存太多的信息和先前请求过的元数据信息，这就需要客户端每次都使用多个首部字段来描述所传输的资源及其属性，又由于HTTP1.1是以文本形式传输的，每次都会给HTTP报文增加500-800字节的负荷，如果算上cookie，这个负荷甚至会增加到上千。如果可以有办法减少这些开销，那么对性能又有很大的提升。<ol><li>HTTP2采用HPACK压缩方法，一边用index mapping table压缩，一边编码。这个table由<a href="https://http2.github.io/http2-spec/compression.html#static.table.definition" target="_blank" rel="noopener">静态表</a>和动态表组成。<ol><li>先用预定义的index mapping table将头部中常用的字符串用index来代替</li><li>对一定要使用文本表示的字符串，采用预定义的哈夫曼编码表进行编码 (具体的压缩和解压缩的方法请看<a href="http://www.cnblogs.com/ghj1976/p/4586529.html" target="_blank" rel="noopener">此文</a>的解释)<ul><li>客户端和服务器端使用首部表来跟踪和存储之前发送的每一个键值对。第一次请求时发送过的键值对，第二次在再请求时就不在发送了。（这一现象还一直没观察到）</li><li>在tcp连接期间，客户端和服务端共同维护这个首部表，并且是共同渐进更新的</li><li>每个新的键值对，要么直接添加到首部表尾部，要么替换原有表中的值</li></ul></li></ol></li><li>另外，HTTP2的前身SPDY采用的头部压缩算法是delate算法，这个算法的压缩效率也不错，但是由于存在<a href="http://www.freebuf.com/articles/web/5636.html" target="_blank" rel="noopener">CRIME攻击</a>，而HTTP2不得不重新设计了HPACK算法。</li></ol></li><li><strong>解释</strong> 使用了HPACK算法，一方面如果下一次请求头部和上一次请求头部中有相同的字段，那么相同的字段不会被发送，只会被发送差异性的字段。另一方面，会有一张首部表，里面会有常用的首部字段极其对应的序号，会有序号来代替这个具体的字段字符串。同时，整个首部帧还会用哈夫曼编码来进行压缩。</li></ul><h4 id="多路复用"><a href="#多路复用" class="headerlink" title="多路复用"></a><strong>多路复用</strong></h4><ul><li><p>引入二进制分帧层之后，HTTP2.0可以很容易的去实现多流并行而不用依赖建立多个tcp连接。</p><blockquote><p>实验表明，客户端使用更少的连接肯定可以降低延迟时间。HTTP 2.0 发送的总分组数量比 HTTP 差不多要少 40%。而服务器处理大量并发连接的情况也变成了可伸缩性问题，因为 HTTP 2.0 减轻了这个负担。 —-HTTP2.0</p></blockquote></li><li><p>每个来源使用一个连接，优势如下：</p><ul><li>从服务器和客户端来说，占用的资源和内存都少了。</li><li>从tcp连接和网络来说，使得网络拥塞得到改善，慢启动时间减少，拥塞和丢包恢复速度变快。</li></ul></li><li><p><strong>解释</strong></p><ul><li>keep-alive，也是可以不进行3次握手就可以发送多个在同一个域下的请求。但是必须发送下一个请求B，等待前一个请求A的响应收到后才可以发送。多路复用和管道优化差不多，只是管道优化由于兼容性的问题，而没有被普遍使用。</li><li>但是什么时候断？如果太多的保持连接，会不会反而不好呢 和一般TCP连接释放一样，如果客户端没有数据要请求，或服务端数据发送完毕后，会主动发送关闭连接的报文。或者是服务端连续发送10个探测报文，客户端无响应，服务端就关闭了这个连接。</li></ul></li></ul><h4 id="请求优先级"><a href="#请求优先级" class="headerlink" title="请求优先级"></a><strong>请求优先级</strong></h4><ul><li><p>当同一条连接上可以同时发送很多请求时，并不等于说这些请求对于服务器来说都应该一视同仁，因为客户端对资源的需求程度不同。比如说一个html文档，显然客户端对CSS和JS的需求，远大于对文档内图片的需求。</p><p>因此在建立新流时，HEADER帧可以带有一个优先级(31位，0为优先级最高)的值。这样，服务端就可以因此而适当分配资源，优先发送这些优先级高的帧。</p><p>HTTP2.0协议并没有规定这样的处理优先级的算法该如何实现，仅仅只是提供了这样一种机制。</p><p>为了合理充分利用网络资源，服务器也应该交错处理不同优先级的帧。而不是严格按照优先级来处理，否则又会造成队首堵塞的情况。</p></li><li><p><strong>解释</strong> ：请求优先级….感觉需要服务器也要支持的节奏（那确实变复杂了） 是的。也就是说服务器和客户端对这个优先级的理解是一样的，或是达成一致的。</p></li></ul><h4 id="服务器推送"><a href="#服务器推送" class="headerlink" title="服务器推送"></a><strong>服务器推送</strong></h4><ul><li><p>说到服务器推送，其实在HTTP1.1时，我们就用到过类似的，比如将图片使用base64编码嵌入在文档中。</p><p>之所以要提供这个服务，是因为一个文档被请求回来时，往往还需要再次请求很多文档内的其他资源，如果这些资源的请求不用客户端发起，而是服务端提前预判发给客户端，那么就会减少一半的RTT。</p><p>HTTP2.0这个协议也没有规定服务器端到底该怎样推送这个资源。服务端可以自己制定不同的策略，可以是根据客户端明确写出的推送请求；或者是服务端通过学习得来；再或者是通过额外的HTTP首部想服务端表明意向。</p></li><li><p>这个服务的特点是：</p><ul><li>只有建立连接后，服务器才可以推送资源(发送PUSH_PROMISE帧，这个帧中只有要约的HTTP首部)，也就是说服务器不能无缘无故的主动向客户端推送资源</li><li>客户端可以发送RST_STREAM拒绝服务器推送来的资源。但是这可能存在一个时间差，而导致客户端明明已经拒绝了，但服务端却还是把资源推送了过来。</li><li>推送的资源可以有不同页面共享</li><li>服务器可以按照优先级来推送资源</li></ul></li><li><p><strong>解释</strong> 服务器主动推送一个资源，客户端有权来选择是接收还是不接收，不能‘来者不拒’嘛，是吧。</p></li></ul><h4 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a><strong>流量控制</strong></h4><ul><li>我们知道在HTTP2协议中，我们可以在同一个连接中，建立多个流，那么实际上这些流之间是相互竞争的，会相互争夺这个连接中资源的分配。此时与tcp流量控制相似，我们也需要对流中的帧进行流量控制。但只有DATA帧受流量控制，而其他类型的帧不受流量控制。同样，HTTP2也只提供了这样一种机制，而非具体实现。<ul><li>这个流量控制在没一跳之间进行，而非端到端</li><li>流量控制基于窗口更新帧。连接建立之初，通过交换settings帧，来设定双向的流量控制窗口大小。</li><li>发送端每发送一个DATA帧，就将window的值减去这次data帧的大小，直到window=0。</li><li>接收方可通过发送window_update 帧。如果接收方不想接受数据了，就不发送window _update帧。</li><li>在接受关键资源时，可将非关键资源的window设置的非常小，等网络空闲了，再改回大一些。</li></ul></li></ul><h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><ul><li><p>可以使用chrome的工具<code>chrome://net-internals/#http2</code>来查看具体发送的帧的内容</p></li><li><p>打开chrome-network中的protocol一栏，查看当前站点使用的HTTP版本</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/chrome-tool.png" alt=""></p></li><li><p>安装chrome扩展<a href="https://chrome.google.com/webstore/detail/http2-and-spdy-indicator/mpbpobfflnpcgagjijhmgnchggcjblin?hl=zh-CN" target="_blank" rel="noopener">HTTP/2 and SPDY indicator</a>,在地址栏右侧会标示出是否使用了HTTP2或SPDY协议。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/chrome-tool1.png" alt=""></p></li><li><p>在firefox浏览器的网络中，也可直接查看使用的HTTP协议的版本</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/firefox-tool.png" alt=""></p></li><li><p>帮助我们检测某个网站是否使用了HTTP2协议的网站<a href="https://www.h2check.org/" target="_blank" rel="noopener">HTTP/2 Checker</a>;检测是否使用了SPDY协议的网站<a href="https://spdycheck.org/" target="_blank" rel="noopener">SPDYCheck.org</a></p></li><li><p>分析页面性能的网站<a href="http://www.webpagetest.org/" target="_blank" rel="noopener">WebPagetest</a></p></li></ul><h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>为了能够亲自证实HTTP2确实对web性能有了很大的改进，使用nodejs作为服务端，分别验证在使用HTTP2, HTTPS, HTTP和SPDY作为HTTP协议时，同时加载10张图片时web性能的表现，<a href="http://jiaolonghuang.github.io/code/http2-experiment.zip" target="_blank" rel="noopener">代码下载</a>。结果比较出乎意外：</p><ul><li><p>可以比较直观的观察到多路复用的表现</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/http2-e.png" alt=""></p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/http1-e.png" alt=""></p></li><li><p>Server Push的验证 <a href="https://www.imququ.com/" target="_blank" rel="noopener">屈屈</a>的<a href="https://www.imququ.com/post/use-http2-now.html" target="_blank" rel="noopener">博客</a>中有专门的介绍，这里我们也看一下。打开博客首页后可看到响应头中有<code>link:&lt;...&gt;</code>这样一个键值对，这是告诉服务器这个资源需要被推送。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/server-push.png" alt=""></p><p>然后使用之前提到过的<code>chrome://net-internals/#http2</code>工具来查看具体的Push过程</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/four/push_promise.png" alt=""></p></li><li><p>至于头部压缩字节变少，页面加载速度变慢并无明显结果。与<a href="http://www.infoq.com/cn/news/2015/02/https-spdy-http2-comparison" target="_blank" rel="noopener">HTTPS、SPDY和HTTP/2的性能比较</a>的实验结果差的也比较多。</p></li></ul><p>猜测原因：由于是访问的本地资源，不能模拟网络拥塞的情况，故不能完全体现出http2的优势。</p><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><ol><li>套接字：是支持TCP/IP网络通信的基本操作单元，可以看成是不同主机之间的进程进行双向通信的端点。是应用进程与tcp连接之间的门，通过套接字口来发送或获得报文。</li><li>HTTP队首阻塞：一个慢请求阻塞后面的所有请求。具体来说就是，假设客户端同时发送2个请求，一个高优先级，一个低优先级，即便低优先级的资源先准备好了，也不会先发送，而是先等着，等高优先级的响应发送完了再发送低优先级的。这样会导致网络资源浪费，服务器缓冲开销浪费，最终导致客户端等待时间无限期延迟。</li><li>tcp队首阻塞：tcp要求分组严格按照顺序交付，一个分组未收到，就会阻塞后续的所有高序号分组。直到重传那个丢失的分组。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;HTTP2.0可以让我们的应用变得更快、更简单、更健壮，让我们在HTTP/1.1时针对TCP协议特性而做的用来提高性能的HACK一笔勾销&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了提高应用的性能，降低延迟，我们能做的无外乎2点，要么传输的东西越小越好，要么距离能获得资源的地方越近越好。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP2的目的：通过支持&lt;strong&gt;多路复用&lt;/strong&gt;来提高并行能力，减少因为3次握手等而产生的延迟；通过&lt;strong&gt;压缩HTTP首部&lt;/strong&gt;将协议开销降到最低，同时支持请求优先级和服务器推送。&lt;/li&gt;
&lt;li&gt;HTTP2最大的改变：引入&lt;strong&gt;二进制分帧层&lt;/strong&gt;。HTTP2.0不会改动HTTP1.x的语义，提供的功能也不变，但是HTTP2对内修改了格式化数据的方式，以及传输这些数据的方式。对外，也就是面向应用，不用做任何改变，感知不到这一层的变化的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;其实HTTP2.0是对HTTP1.x的一个扩展，而非替代，之所以称之为2是因为它引入的二进制分帧层之前的HTTP1.x的服务端和客户端并不兼容。&lt;/p&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>HTTP学习（一）——HTTP的历史</title>
    <link href="http://roux.top/2018/04/20/HTTP%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://roux.top/2018/04/20/HTTP学习（一）/</id>
    <published>2018-04-19T16:00:00.000Z</published>
    <updated>2018-04-22T05:44:00.950Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>本系列文章均转自<a href="https://imjiaolong.cn/" target="_blank" rel="noopener">小路口</a>的系列博文，因为没有找到博主的联系方式，故如有侵权烦请联系我进行处理。</li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li>在上世纪60年代，世界上占统治地位的通信网络是电话网络，它是基于电路交换[1]的。与此同时，随着分时计算机的出现和计算机重要性的提高，如何充分利用分散在世界各地的数字计算机为人们所共享就变的越发重要了。其中需要注意的是，这些用户所产生的流量是具有突发性的，也就是说具有活动的间断性。</li><li>于是世界上的科学家们就发明了更能很好解决突发性的分组交换来替代电路交换。最开始是美国的阿帕网，ARPAnet，是世界上第一个分组交换机网络，也就是今天因特网的直接祖先。</li><li>HTTP协议是现代web的基础。其实HTTP协议的出现主要是为了推动万维网的应用，它们的发明者是同一个人。HTTP协议设计之初是非常简洁的，就是为了传输超文本文档，也就是HTTP0.9版本。</li></ul><a id="more"></a><h3 id="HTTP的版本"><a href="#HTTP的版本" class="headerlink" title="HTTP的版本"></a>HTTP的版本</h3><ul><li><p>HTTP0.9</p><ul><li>请求：ASCII字符串＋CRLF结尾</li><li>响应：ASCII字符流/HTML+CRLF结尾</li><li>只能GET获取资源，不能发送携带数据的POST请求</li><li>文档传完之后，断开连接</li></ul></li><li><p>HTTP1.0（并不是真正的规范）</p><p>随着页面上的内容逐渐丰富，除了文字，超链接，还有图片资源。 人们对web应用的需求也不断增加，要求不仅能够访问HTML同时还需要能够提供有关请求和响应的元数据（首部字段），在页面上进行交互操作，比如表单提交，并且支持内容协商。</p><ul><li>请求/响应的首部可有多行首部字段构成</li><li>响应对象前增加了一个响应状态行</li><li>可以支持长连接，还是每次请求结束后默认都断开连接</li></ul></li><li><p>HTTP1.1</p><ul><li>持久连接</li><li>分块传输</li><li>字节范围请求</li><li>协商机制更健全，协商内容更多：内容编码，传输编码，缓存指令，客户端cookie等</li><li>HTTP pipling(实际使用受限，浏览器默认不开启，很多中间代理不提供该服务)</li></ul></li><li><p>HTTP2 在web应用中资源越来越丰富的趋势中，由于tcp自身的特点以至于HTTP1.1的性能上受到了很大的限制，虽然我们有做一些针对性能提高的hack。HTTP2的出现主要是为了提高传输性能，降低延迟，提高吞吐量。</p></li></ul><p>从总体来说，HTTP0.9用了一行命令就启动了万维网，HTTP1.0是对0.9扩展的正式说明，而HTTP1.1则是一份官方标准。总之，HTTP1.x的设计的初衷是实现要简单。但是这也是以牺牲性能为代价的。所以，这也是现在HTTP2所要解决的问题。</p><ul><li><strong>解释</strong><ol><li>“HTTP1.x的设计的初衷是实现要简单。但是这也是以牺牲性能为代价的” 比如没有处理并行连接和请求的快捷方法，没有为了减少不必要的请求开销而做优化（首部压缩）等等，这些以至于在页面资源极其丰富的今天，即便我们针对HTTP1.1做了很多HACK优化，然而性能还是没有得到很大的提升。而这些点在HTTP2中都得到了改进。</li></ol></li></ul><h3 id="HTTP与tcp的关系"><a href="#HTTP与tcp的关系" class="headerlink" title="HTTP与tcp的关系"></a>HTTP与tcp的关系</h3><ul><li>HTTP是应用层的协议，tcp是传输层的协议。HTTP协议并没有规定必须要使用tcp协议当作是传输层的协议，现在默认使用tcp当做是传输层协议，采用80端口（因为HTTP是无状态协议，要是使用未经改良的不可靠传输协议，很容易造成数据丢失）；另外，谷歌还研发了一种基于udp的多路传输协议QUIC(Quick UDP Internet Connections)[2]，用来解决SPDY在TCP遇到的瓶颈而在UDP上做出的探索。</li><li>HTTP是一种无状态的协议，当前的请求与上一次的请求并无关系，也就是说这次HTTP请求做什么和上一次请求做什么是没有关系的，哪怕请求的是相同的资源。这样做的好处是，服务器不用为了保存状态而消耗过多的资源，坏处是重复发相同的状态，会浪费网络资源，造成网络拥塞，延迟增大。</li><li>tcp提供了可靠传输的性能，为精确传输做了优化，比如3次握手，差错检测，快速重传。同时还进行了拥塞预防与控制，慢启动等来减轻整个网络的拥塞程度。一个数据包从请求到收到，需要经过传播延迟，传输延迟，处理延迟和排队延迟。因此，tcp比较适合大块数据的精确传输，性价比高；若常有突发的连接请求，实际上是其负作用的。原因很简单，在拿到完整数据之前，tcp做了太多事，造成了太多延迟，至少1个RTT。</li></ul><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><ol><li>电路交换：在端系统会话过程中，预留了端系统间所经路径所需要的资源，包括缓存，链路传输速率。</li><li>QUIC能够处理传输可靠性、丢包或无序数据包等一系列UDP默认未处理的问题。它的高层类似SPDY，低层是在UDP上模仿实现TCP的面向连接特性和可靠性并加入类似TLS的加密过程</li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://imjiaolong.cn/post/http-history.html" target="_blank" rel="noopener">https://imjiaolong.cn/post/http-history.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;本系列文章均转自&lt;a href=&quot;https://imjiaolong.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;小路口&lt;/a&gt;的系列博文，因为没有找到博主的联系方式，故如有侵权烦请联系我进行处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在上世纪60年代，世界上占统治地位的通信网络是电话网络，它是基于电路交换[1]的。与此同时，随着分时计算机的出现和计算机重要性的提高，如何充分利用分散在世界各地的数字计算机为人们所共享就变的越发重要了。其中需要注意的是，这些用户所产生的流量是具有突发性的，也就是说具有活动的间断性。&lt;/li&gt;
&lt;li&gt;于是世界上的科学家们就发明了更能很好解决突发性的分组交换来替代电路交换。最开始是美国的阿帕网，ARPAnet，是世界上第一个分组交换机网络，也就是今天因特网的直接祖先。&lt;/li&gt;
&lt;li&gt;HTTP协议是现代web的基础。其实HTTP协议的出现主要是为了推动万维网的应用，它们的发明者是同一个人。HTTP协议设计之初是非常简洁的，就是为了传输超文本文档，也就是HTTP0.9版本。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>HTTP学习（二）——一条消息的历程</title>
    <link href="http://roux.top/2018/04/20/HTTP%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://roux.top/2018/04/20/HTTP学习（二）/</id>
    <published>2018-04-19T16:00:00.000Z</published>
    <updated>2018-04-22T05:45:53.227Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>我们都知道HTTP底层还是调用TCP协议进行传输的，那么具体怎样调用的呢（如下图）？在这个过程中一条消息都经历了什么过程呢？<ul><li><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/two/%E5%8E%86%E7%A8%8B.jpg" alt="历程"></li></ul></li></ul><a id="more"></a><h3 id="历程"><a href="#历程" class="headerlink" title="历程"></a>历程</h3><ul><li><p>应用层（HTTP, HTTP报文）</p><ul><li><p>HTTP报文是基于文本的，<strong>是没有长度限制</strong>的。也就是说资源有多大，就会一次性都会发过去。HTTP报文的首部行之间是用换行回车来分隔。另外，发送过去的报文实际内容就是将一个个的字符用ASCII码来表示。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/two/http-layer.png" alt=""></p></li></ul></li><li><p>运输层(TCP，报文段)</p><ul><li>如果TCP需要发送一个很大的数据，TCP通常会把文件化成长度为<strong>MSS</strong>[1]的若干块再发送。也就是将长报文划分成短报文，并提供拥塞控制机制。TCP报文的首部一般是20字节，包括端口号，序号[3]和确认号[4]，以及做验证的一些字段。<ul><li><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/two/tcp-layer.png" alt=""></li></ul></li></ul></li><li><p>网络层(IP, 数据报)</p><ul><li><p>增加了源IP地址和目的IP地址；同时规定了数据报长度（包括首部长），理论上是<strong>65535字节</strong>。</p></li><li><p>有可能从发送方到接受方这一路径中每段链路使用的<strong>链路层协议不同，且这些协议具有不同的MTU</strong>[2],因此就需要对ip数据报进行分片。</p></li><li><p>只有完全重构为初始ip的数据报才会被交付到上层，否则会被直接丢弃。如果运输层是TCP，那么TCP会要求源重传。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/two/ip-layer.png" alt=""></p></li></ul></li><li><p>链路层(帧)</p><ul><li><p>传输的数据量有<strong>固定大小</strong>，不同的链路层协议能传送的最大数据量不同。可见<a href="http://blog.chinaunix.net/uid-20530497-id-2878069.html" target="_blank" rel="noopener">此文</a></p></li><li><p>会增加源mac地址和目的mac地址。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/two/datalink-layer.png" alt=""></p></li></ul></li></ul><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><ol><li>MSS: 最大报文段长度，不包括TCP报文的头部。受最大链路层帧长度<strong>MTU</strong>的限制。</li><li>MTU: 最大传输单元，链路层协议不同，议具的MTU也有可能不同，其中以太网和PPP链路层的MTU是1500字节。</li><li>序号：这个分组的第一字节排在整个消息的第几位。</li><li>确认号：期望下次从主机那儿获得字节的序号。</li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://imjiaolong.cn/post/message-road.html" target="_blank" rel="noopener">https://imjiaolong.cn/post/message-road.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;我们都知道HTTP底层还是调用TCP协议进行传输的，那么具体怎样调用的呢（如下图）？在这个过程中一条消息都经历了什么过程呢？&lt;ul&gt;
&lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/paulRoux/Pictures/master/HTTP/two/%E5%8E%86%E7%A8%8B.jpg&quot; alt=&quot;历程&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>HTTP学习（三）——关于HTTP/1.1的管道化</title>
    <link href="http://roux.top/2018/04/20/HTTP%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://roux.top/2018/04/20/HTTP学习（三）/</id>
    <published>2018-04-19T16:00:00.000Z</published>
    <updated>2018-04-22T05:45:50.220Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是http管道化"><a href="#什么是http管道化" class="headerlink" title="什么是http管道化"></a>什么是http管道化</h3><ul><li>通常，http请求总是顺序发送的，下一个请求只有在当前请求的响应被完全接受的时候才会被发送。由于网络延迟和带宽的限制，这样会导致在服务器发送下一个响应的时候中间有很大的延迟。 </li><li>HTTP/1.1允许多个http请求通过一个套接字同时被输出 ，而不用等待相应的响应。然后请求者就会等待各自的响应，这些响应是按照之前请求的顺序依次到达。（me：所有请求保持一个FIFO的队列，一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出；同时，服务器端返回这些请求的响应时也是按照FIFO的顺序）。<ul><li>管道化的表现可以大大提高页面加载的速度，尤其是在高延迟连接中。 管道化同样也可以减少tcp/ip的数据包。通常MSS的大小是在536-1460字节，所以将许多个http请求放在一个tcp/ip包 里也是有可能的。减少加载一个网页所需数据包的数量可以在整体上对网络有益处，因为数据包越少，路由器和网络带来的负担就越少。</li><li>HTTP/1.1需要服务器也支持管道化。但这并不意味着服务器需要管道化响应，而是当客户端发出管道化请求时，服务器不会响应失败。这显然有可能引入一种新的福音主义错误（原文：This obviously has the potential to introduce a new category of evangelism bugs），因为仅有现代浏览器支持管道化。</li></ul></li></ul><a id="more"></a><h3 id="什么时候我们应该管道化请求"><a href="#什么时候我们应该管道化请求" class="headerlink" title="什么时候我们应该管道化请求"></a>什么时候我们应该管道化请求</h3><ul><li>只有幂等的请求[1] 才可以被管道化，比如GET和HEAD。POST和PUT不应该被管道化。我们同样也不应该在建立新连接的时候发出管道化的请求 ，因为不能确源服务或代理是否支持HTTP/1.1。因此，管道化只能利用已存在的 keep-alive 连接。</li></ul><h3 id="多少个请求应该被管道化"><a href="#多少个请求应该被管道化" class="headerlink" title="多少个请求应该被管道化"></a>多少个请求应该被管道化</h3><ul><li>如果连接过早的关闭，管道化许多请求是划不来的，因为我们会花费很多时间用来向网络里写请求，然后还不得不在新连接中重写一遍。而且，如果较早到达的请求需要花费很长的时间完成，一个过长的管道实际上会让用户感知到更长的延迟。</li><li>HTTP/1.1标准也没有提供关于管道化请求理想数目的任何指导。实际上，我们建议每个服务器不超过2个keep-alive连接。显然，这个还得依赖于应用本身。鉴于上述的原因，浏览器可能不需要一个持续时间特别长的管道。2个可能是比较合适的值，但是还有待测试。</li></ul><h3 id="如果一个请求被取消了，会发生什么？"><a href="#如果一个请求被取消了，会发生什么？" class="headerlink" title="如果一个请求被取消了，会发生什么？"></a>如果一个请求被取消了，会发生什么？</h3><ul><li>如果一请求被取消了，是不是意味着整个管道都被取消了呢？或者，是不是意味着这个被取消请求的响应应该被简单的丢弃，以便这个管道中的其他请求不会被强制重发？<ul><li>这个答案依赖于很多因素，包括，这个被取消请求的响应还有多少没有被收到。最原始的办法可能是简单的取消管道，然后重发所有的请求。仅仅当请求是幂等的时候才可以。这样原始的方法也可以产生好的影响，因为正在管道中被发送的请求可能属于同一个正在被取消的页面载入组。</li></ul></li></ul><h3 id="如果连接失败会发生什么？"><a href="#如果连接失败会发生什么？" class="headerlink" title="如果连接失败会发生什么？"></a>如果连接失败会发生什么？</h3><ul><li>如果连接失败了或服务器在下载一个管道中的响应时中断了，浏览器必须有能力重新开始发送被丢失的请求。这种情况可以等同于上面讨论的被取消的例子。</li></ul><h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><ol><li>其实HTTP管道化就是将客户端的FIFO队列移到了服务端。在客户端可以依次发送所有要发送的请求（当然这些请求是在同一个域下的），一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出。在服务器端维持的FIFO队列，这个队列是按照资源的重要程度排列的。比如HTML比CSS要先返回，JS,CSS比图片先返回。</li><li>在服务器端会有一个缓冲区，来存放那些已经被处理好了但是还没轮到被发送的响应。比如服务器先后收到了A,B两个请求，A资源比B资源优先级要高，处理A需要10ms，处理B需要1ms，假设服务器可以并行处理请求，那么B的响应肯定是最先处理好了的，但是B响应不能先发出去，必须待在缓冲区里，等待A响应处理好了之后，先把A的响应发出去，B的响应才能够被发出去。因为服务端必须要遵循FIFO这个原则。</li><li>HTTP管道化不是HTTP2的内容，是对HTTP1.1协议下，服务器不能很好处理并行请求的一个改进。</li><li>管道化的有序和TCP的有序是本质上的不同，管道化的有序，是消息与消息之间的有序。TCP中的有序，组成一个消息的多个报文段之间的有序。</li><li>管道做了哪些事，我的理解是创造了一个可以不用等待前一个请求的响应即可发送下一个请求的场所。至于注意些什么，除了知道有些设备不支持，其他的我也没实际经验（毕竟没用过，囧）。</li></ol><h3 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h3><ol><li><strong>HTTP/方法的幂等性</strong>：是指一次和多次请求某一个资源应该具有同样的副作用。 幂等性的请求，实际上就是多次操作都不会改变结果的请求，比如GET，我可以多次从同一个地方获取资源，但是对于资源本身来说并不会发生什么变化，我GET10次和GET100次，资源都没有发生任何变化。而post则不同了，我提交表单10次，和100次，造成的结果是不同的，至少数据库里新增的数据有不同。<ul><li>详情见<a href="https://www.jianshu.com/p/475589f5cd7b" target="_blank" rel="noopener">此文</a></li></ul></li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://imjiaolong.cn/post/http-pipelining.html" target="_blank" rel="noopener">https://imjiaolong.cn/post/http-pipelining.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;什么是http管道化&quot;&gt;&lt;a href=&quot;#什么是http管道化&quot; class=&quot;headerlink&quot; title=&quot;什么是http管道化&quot;&gt;&lt;/a&gt;什么是http管道化&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;通常，http请求总是顺序发送的，下一个请求只有在当前请求的响应被完全接受的时候才会被发送。由于网络延迟和带宽的限制，这样会导致在服务器发送下一个响应的时候中间有很大的延迟。 &lt;/li&gt;
&lt;li&gt;HTTP/1.1允许多个http请求通过一个套接字同时被输出 ，而不用等待相应的响应。然后请求者就会等待各自的响应，这些响应是按照之前请求的顺序依次到达。（me：所有请求保持一个FIFO的队列，一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出；同时，服务器端返回这些请求的响应时也是按照FIFO的顺序）。&lt;ul&gt;
&lt;li&gt;管道化的表现可以大大提高页面加载的速度，尤其是在高延迟连接中。 管道化同样也可以减少tcp/ip的数据包。通常MSS的大小是在536-1460字节，所以将许多个http请求放在一个tcp/ip包 里也是有可能的。减少加载一个网页所需数据包的数量可以在整体上对网络有益处，因为数据包越少，路由器和网络带来的负担就越少。&lt;/li&gt;
&lt;li&gt;HTTP/1.1需要服务器也支持管道化。但这并不意味着服务器需要管道化响应，而是当客户端发出管道化请求时，服务器不会响应失败。这显然有可能引入一种新的福音主义错误（原文：This obviously has the potential to introduce a new category of evangelism bugs），因为仅有现代浏览器支持管道化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="HTTP" scheme="http://roux.top/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>TCP流量控制</title>
    <link href="http://roux.top/2018/04/08/TCP%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/"/>
    <id>http://roux.top/2018/04/08/TCP流量控制/</id>
    <published>2018-04-07T16:00:00.000Z</published>
    <updated>2018-04-22T13:00:30.351Z</updated>
    
    <content type="html"><![CDATA[<h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><ul><li>问题：如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。<ul><li>所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。利用滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制。<ul><li>滑动窗口理解起来的话可以想象要发送的数据在一个首尾连接的数组上，而滑动窗口则是这个上面的一部分（大小由收发双方进行协商（协商的过程也就是对数据的发送和确认的过程）动态变化）。</li></ul></li></ul></li></ul><a id="more"></a><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><ul><li>设A向B发送数据。在连接建立时，B告诉了A：“我的接收窗口是 rwnd = 400 ”(这里的 rwnd 表示 receiver window) 。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。<ul><li>请注意，TCP的窗口单位是字节，不是报文段。TCP连接建立时的窗口协商过程在图中没有显示出来。</li><li>设每一个报文段为100字节长，而数据报文段序号的初始值设为1。大写ACK表示首部中的确认位ACK，小写ack表示确认字段的值ack<br><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/TCP/SouthEast.jpg" alt=""></li><li>从图中可以看出，B进行了三次流量控制。第一次把窗口减少到 rwnd = 300 ，第二次又减到了 rwnd = 100 ，最后减到 rwnd = 0 ，即不允许发送方再发送数据了。这种使发送方暂停发送的状态将持续到主机B重新发出一个新的窗口值为止。B向A发送的三个报文段都设置了 ACK = 1 ，只有在ACK=1时确认号字段才有意义。</li><li>TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器。</li></ul></li></ul><h3 id="糊涂窗口综合症（Silly-Window-Syndrome）"><a href="#糊涂窗口综合症（Silly-Window-Syndrome）" class="headerlink" title="糊涂窗口综合症（Silly Window Syndrome）"></a>糊涂窗口综合症（Silly Window Syndrome）</h3><ul><li>解释<ul><li>当发送端应用进程产生数据很慢、或接收端应用进程处理接收缓冲区数据很慢，或二者兼而有之；就会使应用进程间传送的报文段很小，特别是有效载荷很小。极端情况下，有效载荷可能只有1个字节；而传输开销有40字节(20字节的IP头+20字节的TCP头) 这种现象就叫糊涂窗口综合症。</li></ul></li><li>发送端引起的SWS<ul><li>如果发送端为产生数据很慢的应用程序服务(典型的有telnet应用)，例如，一次产生一个字节。这个应用程序一次将一个字节的数据写入发送端的TCP的缓存。如果发送端的TCP没有特定的指令，它就产生只包括一个字节数据的报文段。结果有很多41字节的IP数据报就在互连网中传来传去。</li><li>解决方法<ul><li>为了防止发送端的TCP逐个字节地发送数据。必须强迫发送端的TCP收集数据，然后用一个更大的数据块来发送。发送端的TCP要等待多长时间呢？如果它等待过长，它就会使整个的过程产生较长的时延。如果它的等待时间不够长，它就可能发送较小的报文段，于是，Nagle找到了一个很好的解决方法，发明了Nagle算法。而他选择的等待时间是一个RTT,即下个ACK来到时。</li></ul></li></ul></li><li>接收端引起的SWS<ul><li>接收端的TCP可能产生糊涂窗口综合症，如果它为消耗数据很慢的应用程序服务，例如，一次消耗一个字节。假定发送应用程序产生了1000字节的数据块，但接收应用程序每次只吸收1字节的数据。再假定接收端的TCP的输入缓存为4000字节。发送端先发送第一个4000字节的数据。接收端将它存储在其缓存中。现在缓存满了。它通知窗口大小为零，这表示发送端必须停止发送数据。接收应用程序从接收端的TCP的输入缓存中读取第一个字节的数据。在入缓存中现在有了1字节的空间。接收端的TCP宣布其窗口大小为1字节，这表示正渴望等待发送数据的发送端的TCP会把这个宣布当作一个好消息，并发送只包括一个字节数据的报文段。这样的过程一直继续下去。一个字节的数据被消耗掉，然后发送只包含一个字节数据的报文段。</li><li>解决方法<ul><li>第一个解决方法是只要有数据到达就发送确认，但宣布的窗口大小为零，直到或者缓存空间已能放入具有最大长度的报文段，或者缓存空间的一半已经空了。</li><li>延迟确认。第二个解决方法是延迟一段时间后再发送确认。这表示当一个报文段到达时并不立即发送确认。接收端在确认收到的报文段之前一直等待，直到入缓存有足够的空间为止。延迟的确认防止了发送端的TCP滑动其窗口。当发送端的TCP发送完其数据后，它就停下来了。这样就防止了这种症状。<ul><li>优点：它减少了通信量。接收端不需要确认每一个报文段。</li><li>缺点，就是迟延的确认有可能迫使发送端重传其未被确认的报文段。可以用协议来平衡这个优点和缺点，例如现在定义了确认的延迟不能超过500毫秒。</li></ul></li></ul></li></ul></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://blog.csdn.net/hzhsan/article/details/46429749" target="_blank" rel="noopener">https://blog.csdn.net/hzhsan/article/details/46429749</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;滑动窗口&quot;&gt;&lt;a href=&quot;#滑动窗口&quot; class=&quot;headerlink&quot; title=&quot;滑动窗口&quot;&gt;&lt;/a&gt;滑动窗口&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;问题：如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。&lt;ul&gt;
&lt;li&gt;所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。利用滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制。&lt;ul&gt;
&lt;li&gt;滑动窗口理解起来的话可以想象要发送的数据在一个首尾连接的数组上，而滑动窗口则是这个上面的一部分（大小由收发双方进行协商（协商的过程也就是对数据的发送和确认的过程）动态变化）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="TCP" scheme="http://roux.top/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>TCP问答</title>
    <link href="http://roux.top/2018/02/08/TCP%E9%97%AE%E7%AD%94/"/>
    <id>http://roux.top/2018/02/08/TCP问答/</id>
    <published>2018-02-07T16:00:00.000Z</published>
    <updated>2018-04-23T14:31:28.717Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h3><ul><li><p>A进程通过TCP向另一台机器上的B进程发送了一个字符串“hello”，TCP返回对方成功接收的确认信息，请问，现在进程A是否可以肯定地说进程B收到了它发送的字符串？</p><ul><li>不能！举反例，进程B所在机器的TCP收到进程A发送的“hello”信息后，告诉进程A发送成功，但有可能没有立即将数据交给进程B，而是放在自己的缓冲区中，等待进程B读取，如果机器此时突然掉电，缓冲区中的信息将丢失，进程B将不可能收到“hello”字符串。</li></ul></li><li><p>有什么办法来尽量避免上述情况的发生呢？</p><ul><li>将TCP报文段首部中的PSH标志置1，这样会让B端的TCP协议收到数据后尽快交给进程B，能不缓存尽量不要缓存。</li></ul></li><li><p>我们知道通常TCP连接的建立需要3次握手，关闭需要4次握手，为什么关闭会多一次呢？</p><ul><li>简单说，就是TCP允许半关闭状态的存在。当进程A向进程B发送FIN，B也向A发送确认后。此时此刻的状态就是半关闭状态，A发送的FIN就是告诉B：“我要发送的数据都发送完了”但B没有发送FIN给A，有可能代表B还有没发送完的数据，如果B也发送完数据了，B就发送FIN给进程A，进程A确认B发送的FIN，这时，双方都已经发送完了数据，连接就断开了，TCP回收相关资源。</li></ul></li><li><p>假如服务器突然断电重启，但客户端并不知情，请问此时二者之间的TCP连接处于什么状态？</p><ul><li>处于半打开状态。就是客户端还觉得连接是正常的，服务器这边已经没有连接的任何信息了。</li></ul></li><li><p>那么，假如此时客户端通过这个连接向服务器请求数据，服务器会怎么应对呢？</p><ul><li>服务器收到客户端的请求会进行一次ARP查询，获得客户端MAC地址，然而由于已经丢失了所有连接信息，此时的服务器是一脸懵逼（就像喝了孟婆汤！），于是乎，它会发一个RST给客户端，表示：“哥们，我不认识你，想跟我说话请先发送SYN！”</li></ul></li><li><p>假如客户端按照服务器的要求重新建立连接，却搞错了服务器的端口号，会发生什么情况呢？</p><ul><li>有两种可能，一种是服务器端的TCP收到客户端请求，查看本机上是否有进程在监听相应的端口，如果有，就把请求交给这个进程，一般而言，这个进程不会接受这个连接的，于是它会发一个RST给客户端。还有一种可能是TCP没有找到哪个进程在监听相应的端口，于是TCP就会直接发一个RST给客户端，一般而言都会是这种情况。</li></ul></li><li><p>假如现在有一个多进程服务器，服务器进程为A,接受一个连接后新建一个进程B来处理连接，再接受一个连接后又建一个进程C来处理这个连接，请问，进程ABC是否监听同样的端口？</p><ul><li>是！因为一条连接由唯一的四元组组成。</li></ul></li><li><p>那TCP接收到发送给这个端口的报文段，怎么决定发给哪个进程呢？</p><ul><li>首先，所有的SYN报文段都会发送给服务器进程A，其他的报文段依据\&lt;sourceIP:port, targetIP:port&gt;这个四元组来决定发送给进程B还是进程C。</li></ul></li><li><p>假如上面的服务器进程A收到一个连接请求，正在为这个请求创建处理进程的时候，又有新的连接请求进来了，TCP会怎么处理呢？</p><ul><li>一般情况下，服务器进程A会给TCP一个指示，让TCP维护一个适当长度的连接队列，TCP与新连接请求完成三次握手后，就会把这个连接放入连接队列中，服务器进程A会在合适的时候来从这个队列中取连接。</li></ul></li><li><p>这个连接对列是否会对服务器的并发处理能力产生影响呢？如果会，会有什么影响？</p><ul><li>不会！二者没有必然关系。</li></ul></li><li><p>MSS和MTU各是什么，二者是什么关系？</p><ul><li>MSS是TCP最大报文段长度，就是TCP发送数据需要对数据分段时，最大的段的字节数。MTU是最大传输单元，通常由网卡的硬件特性规定，表示通过该网卡传输的数据单元最大的字节数。MSS要受同一台机器上的MTU限制。比如MTU为1500字节，那么MSS就只能是1460字节，这是因为1460字节的数据在通过网卡向外传输时，会加上20字节的ip头和20字节的tcp头。</li></ul></li><li><p>假设机器A和B的MSS分别是1400和1600，请问，A通过TCP向B发送数据时，是否可以发送长度为1600字节的数据段？</p><ul><li>不可以，虽然发送1600字节的数据段没有突破B的MSS，但是突破了A自己的MSS。这样一来，当这1600字节的数据段通过A的网卡向B发送时，会被切分为2个IP片，每个为840字节，以保证不突破A的MTU，这显然降低了传输的效率，因为两个840字节中有着相同的IP头和TCP头。</li></ul></li><li><p>机器A和B有一条TCP连接，假如A想尽快断开连接，应当怎么办？</p><ul><li>A可以直接给B发送一个RST，就可以了，相当于告诉B，我关闭连接了，你看着办吧。这叫做异常关闭。</li></ul></li><li><p>B的TCP收到A发来的RST，会怎么办？</p><ul><li>B的TCP会告诉上层的进程，连接已经断开了，然后就会回收这条连接的资源，并不会发送任何确认信息给A。所谓你无情休怪我不义。</li></ul></li><li><p>假设A正常断开与B的TCP连接，当收到B的FIN时，A发送ACK给B，是否就算完成了4次握手，连接已经成功断开？</p><ul><li>不是，A的TCP会启动一个定时器，等待2MSL的时间，这主要是为了防止A的ACK没有成功传到B，让B以为自己的FIN没有送到A处，反复重传FIN的问题。2MSL的时间到时，如果A没有再次收到B的FIN，说明B成功收到A的ACK，A就可以安全地断开这个连接，若期间再次收到B的FIN，则A会重传ACK。</li></ul></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://mp.weixin.qq.com/s/z0eALQt40Wl2xFGyAqY_Uw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/z0eALQt40Wl2xFGyAqY_Uw</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;问答&quot;&gt;&lt;a href=&quot;#问答&quot; class=&quot;headerlink&quot; title=&quot;问答&quot;&gt;&lt;/a&gt;问答&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A进程通过TCP向另一台机器上的B进程发送了一个字符串“hello”，TCP返回对方成功接收的确认信息，请问，现在进程A是
      
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="TCP" scheme="http://roux.top/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>信号量与自旋锁的区别</title>
    <link href="http://roux.top/2018/01/08/%E4%BF%A1%E5%8F%B7%E9%87%8F%E5%92%8C%E8%87%AA%E6%97%8B%E9%94%81%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://roux.top/2018/01/08/信号量和自旋锁的区别/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-04-22T05:54:24.671Z</updated>
    
    <content type="html"><![CDATA[<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><ul><li><p>区别一：实现方式</p><ul><li><p>自旋锁是自旋等待，进程状态始终处于<code>TASK_RUNNING</code>。</p></li><li><p>信号量是睡眠等待，进程在等待时处于<code>TASK_INTERRUPTIBLE</code>。</p></li></ul></li></ul><a id="more"></a><ul><li><p>区别二：睡眠死锁陷阱：</p><ul><li><p>在自旋锁的临界区中，进程是不能陷入睡眠的。</p></li><li><p>信号量可以睡眠。</p></li><li><p>同时，基于上面的原因，中断上下文中只能使用自旋锁（中断里不能休眠），在有睡眠代码的临界区只能使用信号量。</p></li></ul></li><li><p>区别三：CPU的使用情况：</p><ul><li>信号量对系统的负载小，因为它睡眠等待。</li></ul></li><li><p>区别四：执行的效率方面：</p><ul><li><p>自旋锁的效率比较高，因为它少了进程状态切换的消耗。</p></li><li><p>信号量的效率比较低，因为进程的等待需要切换进程状态。</p></li></ul></li><li><p>区别五：上锁的时间长短：</p><ul><li><p>因为自旋锁是忙等待，所以临界区的代码不能太长。</p></li><li><p>信号量可以使用在运行时间较长的临界区代码。</p></li></ul></li><li><p>区别六：是否关抢占：</p><ul><li><p>自旋锁是关抢占的，所以在单处理器非抢占的内核下，自旋锁是没用的，是空操作。</p></li><li><p>信号量并没有关抢占，所以，只有需要获得锁的进程才会睡眠，其他进程还可以继续运行。</p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;区别&quot;&gt;&lt;a href=&quot;#区别&quot; class=&quot;headerlink&quot; title=&quot;区别&quot;&gt;&lt;/a&gt;区别&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;区别一：实现方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;自旋锁是自旋等待，进程状态始终处于&lt;code&gt;TASK_RUNNING&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;信号量是睡眠等待，进程在等待时处于&lt;code&gt;TASK_INTERRUPTIBLE&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="网络编程" scheme="http://roux.top/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="锁" scheme="http://roux.top/tags/%E9%94%81/"/>
    
  </entry>
  
  <entry>
    <title>TIME_WAIT状态</title>
    <link href="http://roux.top/2018/01/08/TIME_WAIT/"/>
    <id>http://roux.top/2018/01/08/TIME_WAIT/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-04-22T05:43:43.299Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><ul><li>关于TCP的<code>TIME_WAIT</code>，一直停留在明白有什么作用的层次。想着《计算机网络》都学完了，也应该把TCP的问题给实实在在的弄明白。</li></ul><a id="more"></a><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><ul><li><p>产生的原因见<a href="http://roux.top/2017/10/10/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/">TCP四次挥手</a></p></li><li><p>一些问题及解决</p><ul><li><p><a href="http://blog.csdn.net/dog250/article/details/13760985" target="_blank" rel="noopener">TIME_WAIT快速回收与重用</a></p></li><li><p><a href="https://huoding.com/2012/01/19/142" target="_blank" rel="noopener">记一次TIME_WAIT网络故障</a></p></li><li><p><a href="http://www.cnblogs.com/yjf512/p/5327886.html" target="_blank" rel="noopener">也说说TIME_WAIT状态</a></p></li></ul></li><li><p>深入剖析</p><ul><li><p><a href="https://huoding.com/2013/12/31/316" target="_blank" rel="noopener">再叙TIME_WAIT</a></p></li><li><p><a href="http://blog.oldboyedu.com/tcp-wait/" target="_blank" rel="noopener">你所不知道的TIME_WAIT和CLOSE_WAIT</a>—-强烈推荐</p></li></ul></li><li><p>以前也和前面文章里说的一样，只是知道出现大量的<code>TIME_WAIT</code>就应该调整那么三个参数就行了，但是没想到还有问题的存在，并且调整参数还会分情况与场景。以后学习要注意！</p></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>关于TCP的学习就基本上结束了，也学了近一年时间了，大大小小的问题在自己的搜索和老师的解答下也基本上弄明白了，部分细节后面慢慢的再深入探索。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;关于TCP的&lt;code&gt;TIME_WAIT&lt;/code&gt;，一直停留在明白有什么作用的层次。想着《计算机网络》都学完了，也应该把TCP的问题给实实在在的弄明白。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="TCP" scheme="http://roux.top/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>TCP拥塞控制</title>
    <link href="http://roux.top/2018/01/08/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/"/>
    <id>http://roux.top/2018/01/08/TCP拥塞控制/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-04-22T12:27:20.775Z</updated>
    
    <content type="html"><![CDATA[<h3 id="拥塞控制与流量控制"><a href="#拥塞控制与流量控制" class="headerlink" title="拥塞控制与流量控制"></a>拥塞控制与流量控制</h3><ul><li><p>拥塞控制：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素</p></li><li><p>流量控制：指点对点通信量的控制，是端到端正的问题。流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收</p></li><li><p>拥塞控制代价：需要获得网络内部流量分布的信息。在实施拥塞控制之前，还需要在结点之间交换信息和各种命令，以便选择控制的策略和实施控制。这样就产生了额外的开销。拥塞控制还需要将一些资源分配给各个用户单独使用，使得网络资源不能更好地实现共享。</p></li></ul><a id="more"></a><h3 id="几种拥塞控制方法"><a href="#几种拥塞控制方法" class="headerlink" title="几种拥塞控制方法"></a>几种拥塞控制方法</h3><ul><li><p>慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。</p></li><li><p><strong>慢开始和拥塞避免</strong></p><ul><li><p>发送方维持一个拥塞窗口 cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口大小</p></li><li><p>发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数</p></li><li><p><strong>慢开始</strong></p><ul><li>慢开始算法：当主机开始发送数据时，如果立即所大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。因此，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。通常在刚刚开始发送报文段时，先把拥塞窗口 cwnd 设置为<strong>一个最大报文段MSS</strong>的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口 cwnd ，可以使分组注入到网络的速率更加合理<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/TCP/cwnd.jpg" alt=""></li><li>每经过一个传输轮次，拥塞窗口 cwnd 就加倍。一个传输轮次所经历的时间其实就是往返时间RTT。不过“传输轮次”更加强调：把拥塞窗口cwnd所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认</li><li><strong>注意：</strong>慢开始的“慢”并不是指cwnd的增长速率慢，而是指在TCP开始发送报文段时先设置cwnd=1，使得发送方在开始时只发送一个报文段（目的是试探一下网络的拥塞情况），然后再逐渐增大cwnd</li><li>为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量（如何设置ssthresh）。慢开始门限ssthresh的用法如下： 当 cwnd &lt; ssthresh 时，使用上述的慢开始算法<ul><li>当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法</li><li>当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法</li></ul></li></ul></li><li><p><strong>拥塞避免算法</strong></p><ul><li>让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多</li><li>无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕</li><li>如下图，用具体数值说明了上述拥塞控制的过程。现在发送窗口的大小和拥塞窗口一样大<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/TCP/%E6%85%A2%E5%BC%80%E5%A7%8B.jpg" alt=""><ol><li>当TCP连接进行初始化时，把拥塞窗口cwnd置为1。前面已说过，为了便于理解，图中的窗口单位不使用字节而使用报文段的个数。慢开始门限的初始值设置为16个报文段，即 cwnd = 16</li><li>在执行慢开始算法时，拥塞窗口 cwnd 的初始值为1。以后发送方每收到一个对新报文段的确认ACK，就把拥塞窗口值另1，然后开始下一轮的传输（图中横坐标为传输轮次）。因此拥塞窗口cwnd随着传输轮次按指数规律增长。当拥塞窗口cwnd增长到慢开始门限值ssthresh时（即当cwnd=16时），就改为执行拥塞控制算法，拥塞窗口按线性规律增长</li><li>假定拥塞窗口的数值增长到24时，网络出现超时（这很可能就是网络发生拥塞了）。更新后的ssthresh值变为12（即变为出现超时时的拥塞窗口数值24的一半），拥塞窗口再重新设置为1，并执行慢开始算法。当cwnd=ssthresh=12时改为执行拥塞避免算法，拥塞窗口按线性规律增长，每经过一个往返时间增加一个MSS的大小</li><li>强调：“拥塞避免”并非指完全能够避免了拥塞。利用以上的措施要完全避免网络拥塞还是不可能的。“拥塞避免”是说在拥塞避免阶段将拥塞窗口控制为按线性规律增长，使网络比较不容易出现拥塞</li></ol></li></ul></li></ul></li><li><p><strong>快重传和快恢复</strong></p></li><li><p>不使用快重传的情况：如果发送方设置的超时计时器时限已到但还没有收到确认，那么很可能是网络出现了拥塞，致使报文段在网络中的某处被丢弃。这时，TCP马上把拥塞窗口 cwnd 减小到1，并执行慢开始算法，同时把慢开始门限值ssthresh减半。</p><ul><li><p><strong>快重传算法</strong></p><ul><li>快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认<br>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/TCP/%E5%BF%AB%E9%87%8D%E4%BC%A0.jpg" alt=""><ul><li>接收方收到了M1和M2后都分别发出了确认。现在假定接收方没有收到M3但接着收到了M4。显然，接收方不能确认M4，因为M4是收到的失序报文段。根据可靠传输原理，接收方可以什么都不做，也可以在适当时机发送一次对M2的确认。</li><li>但按照快重传算法的规定，接收方应及时发送对M2的重复确认，这样做可以让发送方及早知道报文段M3没有到达接收方。发送方接着发送了M5和M6。接收方收到这两个报文后，也还要再次发出对M2的重复确认。这样，发送方共收到了接收方的四个对M2的确认，其中后三个都是重复确认。</li><li>快重传算法还规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段M3，而不必继续等待M3设置的重传计时器到期。</li></ul></li></ul></li><li><p><strong>快恢复算法</strong></p><ol><li>当发送方连续收到三个重复确认，就执行“乘法减小”算法，把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。注意：接下去不执行慢开始算法。</li><li>由于发送方现在认为网络很可能没有发生拥塞，因此与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。 下图给出了快重传和快恢复的示意图，并标明了“TCP Reno版本”。 区别：新的 TCP Reno 版本在快重传之后采用快恢复算法而不是采用慢开始算法。<br> <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/TCP/%E6%8B%A5%E5%A1%9E%E9%81%BF%E5%85%8D.jpg" alt=""><ul><li>也有的快重传实现是把开始时的拥塞窗口cwnd值再增大一点，即等于 ssthresh + 3 X MSS 。这样做的理由是：既然发送方收到三个重复的确认，就表明有三个分组已经离开了网络。这三个分组不再消耗网络 的资源而是停留在接收方的缓存中。可见现在网络中并不是堆积了分组而是减少了三个分组。因此可以适当把拥塞窗口扩大了些</li><li>在采用快恢复算法时，慢开始算法只是在TCP连接建立时和网络出现超时时才使用</li><li>接收方根据自己的接收能力设定了接收窗口rwnd，并把这个窗口值写入TCP首部中的窗口字段，传送给发送方。因此，接收窗口又称为通知窗口。因此，从接收方对发送方的流量控制的角度考虑，发送方的发送窗口一定不能超过对方给出的接收窗口rwnd<ul><li>发送方窗口的上限值 = Min [ rwnd, cwnd ]</li><li>当rwnd &lt; cwnd 时，是接收方的接收能力限制发送方窗口的最大值。<br>当cwnd &lt; rwnd 时，则是网络的拥塞限制发送方窗口的最大值。</li></ul></li></ul></li></ol></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;拥塞控制与流量控制&quot;&gt;&lt;a href=&quot;#拥塞控制与流量控制&quot; class=&quot;headerlink&quot; title=&quot;拥塞控制与流量控制&quot;&gt;&lt;/a&gt;拥塞控制与流量控制&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拥塞控制：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;流量控制：指点对点通信量的控制，是端到端正的问题。流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;拥塞控制代价：需要获得网络内部流量分布的信息。在实施拥塞控制之前，还需要在结点之间交换信息和各种命令，以便选择控制的策略和实施控制。这样就产生了额外的开销。拥塞控制还需要将一些资源分配给各个用户单独使用，使得网络资源不能更好地实现共享。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Protocol" scheme="http://roux.top/categories/Protocol/"/>
    
    
      <category term="TCP" scheme="http://roux.top/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>进程与线程</title>
    <link href="http://roux.top/2017/12/13/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/"/>
    <id>http://roux.top/2017/12/13/进程与线程/</id>
    <published>2017-12-12T16:00:00.000Z</published>
    <updated>2018-04-22T05:56:45.870Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><ul><li>网络编程也学了挺长时间了，想着把最基本的进程和线程捋一捋，为后面的进阶把基础弄扎实点。本文是我们学习交流发的，也不知道出处，在此感谢！文末有两篇文章讲解的很棒，结合起来细细品读，进线程基本就ok了！</li></ul><a id="more"></a><h4 id="进程与线程的区别和联系"><a href="#进程与线程的区别和联系" class="headerlink" title="进程与线程的区别和联系"></a>进程与线程的区别和联系</h4><ul><li><p>进程：</p><ul><li><p>进程是并发执行的程序在执行过程中分配和管理资源的基本单位,是一个动态的概念,操作系统分配资源的基本单位。</p></li><li><p>每一个进程都有自己的空间，在内存中有其完备的数据空间和代码空间，即进程空间或(虚空间).一个进程所拥有的数据和变量只属于它自己。</p></li></ul></li><li><p>线程：</p><ul><li><p>在网络或多用户环境下，一个服务器通常需要接收大量且不确定数量用户的并发请求，为每一个请求都创建一个进程显然是行不通的，——无论是从系统资源开销方面或是响应用户请求的效率方面来看。因此，操作系统中线程的概念便被引进了。线程，是进程的一部分，一个没有线程的进程可以被看作是单线程的。</p></li><li><p>线程有时又被称为轻权进程或轻量级进程，也是 CPU 调度的一个基本单位。（最简单的比喻多线程就像火车的每一节车厢，而进程则是火车。车厢离开火车是无法跑动的，同理火车也不可能只有一节车厢。多线程的出现就是为了提高效率。）</p></li></ul></li><li><p>区别：</p><ul><li><p>进程和线程的主要差别在于它们是操作系统不同的资源管理方式.</p></li><li><p>进程有独立的地址空间,一个进程崩溃后,在保护模式下不会对其他进程产生影响,而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。</p></li><li><p>进程拥有一个完整的虚拟地址空间，不依赖于线程而独立存在；反之，线程是进程的一部分，没有自己的地址空间，与进程内的其他线程一起共享分配给该进程的所有资源。</p></li><li><p>简而言之：</p><ul><li><p>一个程序至少有一个进程,一个进程至少有一个线程.</p></li><li><p>线程的划分尺度小于进程,使得多线程的程序并发性高</p></li><li><p>另外,进程在执行过程中拥有独立的内存单元,而多线程共享内存,从而极大地提高了程序的运行效率</p></li><li><p>线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。</p></li><li><p>从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。</p></li></ul></li></ul></li><li><p>优缺点：</p><ul><li>线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。同时，线程适合于在SMP(多核处理机)机器上运行，而进程则可以跨机器迁移。</li></ul></li></ul><h4 id="进程池线程池的概念"><a href="#进程池线程池的概念" class="headerlink" title="进程池线程池的概念"></a>进程池线程池的概念</h4><ul><li><p>进程池和线程池很相似，所以只介绍进程池。</p><ul><li><p>进程池是由服务器预先创建的一组进程，这些子进程的数目在3-10个之间<code>.http守护进程</code>就是使用了包含7个子进程的进程池来实现并发的。线程池中的线程数量和CPU的数量差不多。</p></li><li><p>进程池中的所有子进程都运行着相同的代码，并具有相同的属性，比如优先级，PGID等。因为进程池在服务器启动之初就创建好了，所以他们不用打开不必要的文件描述符（从父进程继承下来的），也不会错误的使用大块的堆内存（复制父进程的）</p></li><li><p>当有新的任务到来时，主进程将通过某种方式选择进程池中的某一个进程来为之服务。相对于动态创建子进程，选择一个已经存在的子进程的代价明显要小的多。</p></li></ul></li><li><p>关于创建池的数量</p><ul><li><p>CPU密集型任务主要消耗大量CPU资源，大量地进行计算。由于依靠CPU性能，一直占用CPU进行计算，也就说一般情况下能够采用多任务的数量等于CPU核心数。</p></li><li><p>IO密集型任务（磁盘读取，web服务）主要需要IO的读取，利用CPU的效率较低，大量时间花费在IO上。由于现在有异步IO技术，也就是说一个任务在IO等待时间时可以暂停运行，让CPU的空闲时间用来运行其他任务，等到IO读取完毕后在继续执行。从而实现单核CPU上运行单进程却能实现多任务的并发，此时线程数量应大于CPU的数量</p></li><li><p>具体:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C: CPU核心数    N：进程或线程数</span><br><span class="line"></span><br><span class="line">C &lt;= N &lt;= M*C+n    （1 &lt;= M &lt;= 7)    n&lt;C</span><br><span class="line"></span><br><span class="line">M   进程：约3～4，线程：7～8</span><br></pre></td></tr></table></figure></li></ul></li><li><p>进程池与线程池的区别</p><ul><li><p>进程池</p><ul><li><p>优点：健壮性高，操作系统以进程为单位去分配资源</p></li><li><p>缺点：</p><ul><li><p>占用资源高</p></li><li><p>进程间通讯代价大</p></li><li><p>进程切换需要保护上下文，开销大</p></li></ul></li></ul></li><li><p>线程池</p><ul><li><p>优点：轻量级，通信代价小,线程间切换开销小。</p></li><li><p>缺点：健壮性不高，共享资源使用不当会导致错误。</p></li></ul></li></ul></li><li><p>子进程选择算法</p><ul><li><p>主进程通过某种算法主动选择子进程。最简单、最常用的算法是轮流选取算法。</p></li><li><p>主进程和所有的子进程都在同一个共享的队列中，刚开始都处于睡眠状态，当有新的任务到来时，主进程将任务添加到消息队列中。将会通过算法来使其中一个子进程唤醒，这个子进程将任务取出并执行，而其他进程还处于睡眠状态。</p></li><li><p>当选择了一个子进程之后，主进程还要告诉子进程执行任务所需要的数据，而传递这些数据就要在子进程和主进程之间创建一个管道。可以将这些数据全都定义为全局变量，而这些数据将会被所有的线程共享。</p></li></ul></li></ul><h4 id="为什么要用进程池（线程池）"><a href="#为什么要用进程池（线程池）" class="headerlink" title="为什么要用进程池（线程池）"></a>为什么要用进程池（线程池）</h4><ul><li><p>一个子进程是通过进程动态内存分配的方式创建的，这种创建子进程的方式虽然能够获得资源，但是是有很多缺点：</p><ul><li><p>动态创建进程是比较耗时间的，这将导致较慢的客户响应。</p></li><li><p>动态创建的子进程（或子线程）通常只用来为一个客户服务。这将导致系统上产生大量的进程（线程），这将使进程间（线程）间切换消耗大量的CPU资源。</p></li><li><p>动态创建出来的子进程（线程）是父进程（线程）的完整映射。当前进程（线程）必须谨慎的管理其分配的文件描述符和堆内存等系统资源，从而使系统的可用资源急剧下降，进而影响服务器性能。</p></li><li><p>由于系统的资源有限，这将是动态创建的进程（线程）数量有限，从而会使响应客户端请求的数量有上线。</p></li></ul></li><li><p>为了有效的解决在大量连接下的客户请求，就要采用一种方式来避开动态创建带来的缺点。进程（线程）池是通过事先划分一块系统资源区，这块区域在系统启动时 就已经创建并初始化了。用户可以直接取得资源，从而避免动态分配资源。</p></li><li><p>运用池的优点：</p><ul><li><p>可以执行大量相对短暂的任务</p></li><li><p>当任务增加的时候能够动态的增加进程池（线程池)中的进程（线程）的数量直到达到一定的阈值。</p></li><li><p>当任务执行完毕时，能够动态的销毁线程（进程）池中的线程（进程）。</p></li><li><p>系统开销少，能够统一管理。</p></li></ul></li></ul><h4 id="SOURCE"><a href="#SOURCE" class="headerlink" title="SOURCE"></a>SOURCE</h4><ul><li><p><a href="http://www.th7.cn/system/lin/201710/230767.shtml" target="_blank" rel="noopener">进线程区别详解</a></p></li><li><p><a href="https://my.oschina.net/cnyinlinux/blog/367910" target="_blank" rel="noopener">线程的实质</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;网络编程也学了挺长时间了，想着把最基本的进程和线程捋一捋，为后面的进阶把基础弄扎实点。本文是我们学习交流发的，也不知道出处，在此感谢！文末有两篇文章讲解的很棒，结合起来细细品读，进线程基本就ok了！&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="网络编程" scheme="http://roux.top/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="进程" scheme="http://roux.top/tags/%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>线程池浅析</title>
    <link href="http://roux.top/2017/12/12/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"/>
    <id>http://roux.top/2017/12/12/线程池/</id>
    <published>2017-12-11T16:00:00.000Z</published>
    <updated>2017-12-12T13:59:26.774Z</updated>
    
    <content type="html"><![CDATA[<h4 id="池"><a href="#池" class="headerlink" title="池"></a>池</h4><ul><li><p>由于服务器的硬件资源“充裕”，那么提高服务器性能的一个很直接的方法就是以空间换时间，即“浪费”服务器的硬件资源，以换取其运行效率。这就是池的概念。池是一组资源的集合，这组资源在服务器启动之初就完全被创建并初始化，这称为静态资源分配。当服务器进入正是运行阶段，即开始处理客户请求的时候，如果它需要相关的资源，就可以直接从池中获取，无需动态分配。很显然，直接从池中取得所需资源比动态分配资源的速度要快得多，因为分配系统资源的系统调用都是很耗时的。当服务器处理完一个客户连接后，可以把相关的资源放回池中，无需执行系统调用来释放资源。从最终效果来看，池相当于服务器管理系统资源的应用设施，它避免了服务器对内核的频繁访问。</p></li><li><p>池可以分为多种，常见的有内存池、进程池、线程池和连接池。</p></li></ul><a id="more"></a><h4 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h4><ul><li><p><a href="https://baike.baidu.com/item/%E7%BA%BF%E7%A8%8B%E6%B1%A0" target="_blank" rel="noopener">线程池</a></p><ul><li>线程池是一种多线程处理形式，处理过程中将任务添加到队列，然后在创建线程后自动启动这些任务。线程池线程都是后台线程。每个线程都使用默认的堆栈大小，以默认的优先级运行，并处于多线程单元中。如果某个线程在托管代码中空闲（如正在等待某个事件）,则线程池将插入另一个辅助线程来使所有处理器保持繁忙。如果所有线程池线程都始终保持繁忙，但队列中包含挂起的工作，则线程池将在一段时间后创建另一个辅助线程但线程的数目永远不会超过最大值。超过最大值的线程可以排队，但他们要等到其他线程完成后才启动。</li></ul></li><li><p>使用线程的问题</p><ul><li><p>在服务器应用程序中，<strong>串行处理机制</strong>通常都无法提供高吞吐率或快速响应性。也有一些例外，如当任务数量很少且执行时间很长时，或当服务器只为单个用户提供服务，并且该客户每次只发出一个请求时，但大多数应用程序并不是按照这种方式来工作的。</p></li><li><p>一般可以通过<strong>为每个请求创建一个新的线程来提供服务</strong>，从而实现更高的响应性，如果每个线程调用的是同一个业务处理方法，那这个方法必须设计为线程安全的。在正常负载情况下，为每个任务分配一个线程的方法能提升串行执行的性能。只要请求的到达速率不超出服务器的请求处理能力，那么这种方法可以同时带来更快的响应性和更高的吞吐率。</p><ul><li><p>在生产环境中，为每个任务分配一个线程 这种方法存在一些缺陷，尤其是当需要创建大量的线程时：</p><ul><li><p>线程生命周期的开销非常高。</p></li><li><p>资源消耗。活跃的线程会消耗系统资源，尤其是内存；大量空闲的线程也会占用许多内存，给垃圾回收器带来压力。而且大量线程在竞争cpu资源时还将产生其他的性能开销。</p></li><li><p>稳定性。在可创建线程的数量上存在一个限制，这个限制值随平台的不同而不同。</p></li></ul></li><li><p>在一定范围内，增加线程可以提高系统的吞吐率，但如果超出了这个范围，再创建更多的线程只会降低程序的执行速度，并且如果过多创建线程整个程序可能崩溃。</p></li></ul></li></ul></li><li><p>为什么用线程池</p><ul><li><p>创建/销毁线程伴随着系统开销，过于频繁的创建/销毁线程，会很大程度上影响处理效率</p><blockquote><p>例如：</p></blockquote><blockquote><p>记创建线程消耗时间T1，执行任务消耗时间T2，销毁线程消耗时间T3</p></blockquote><blockquote><p>如果T1+T3&gt;T2，那么是不是说开启一个线程来执行这个任务太不划算了！</p></blockquote><blockquote><p>正好，线程池缓存线程，可用已有的闲置线程来执行新任务，避免了T1+T3带来的系统开销</p></blockquote></li><li><p>线程并发数量过多，抢占系统资源从而导致阻塞(同时避免创建过多的线程)</p><blockquote><p>我们知道线程能共享系统资源，如果同时执行的线程过多，就有可能导致系统资源不足而产生阻塞的情况</p></blockquote><blockquote><p>运用线程池能有效的控制线程最大并发数，避免以上的问题</p></blockquote></li><li><p>对线程进行一些简单的管理</p><blockquote><p>比如：延时执行、定时循环执行的策略等运用线程池都能进行很好的实现</p></blockquote></li></ul></li></ul><h4 id="线程池的构成"><a href="#线程池的构成" class="headerlink" title="线程池的构成"></a>线程池的构成</h4><ul><li><p>线程池管理器:用于创建并经管线程池</p></li><li><p>工作线程: 线程池中实际履行的线程</p></li><li><p>任务接口: 尽管线程池大多半场景下是用来支撑收集办事器，然则我们将线程履行的任务抽象出来，形成任务接口，从而达到线程池与具体的任务无关。</p></li><li><p>任务队列:线程池的概念具体到实现则可能是队列，链表之类的数据布局，此中保存履行线程。</p></li></ul><h4 id="线程池处理流程"><a href="#线程池处理流程" class="headerlink" title="线程池处理流程"></a>线程池处理流程</h4><ul><li><p>线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的核心工作线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下个流程。</p></li><li><p>线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。</p></li><li><p>线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的非核心工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。</p></li></ul><h4 id="线程池的使用"><a href="#线程池的使用" class="headerlink" title="线程池的使用"></a>线程池的使用</h4><ul><li><p>只有当任务都是同类型的并且相互独立时，线程池的性能才能达到最佳。</p></li><li><p>如果将运行时间较长的与运行时间较短的任务混合在一起，那么除非线程池很大，否则可能造成拥塞，因为很有可能在线程池中运行的都是运行时间较长的任务，其他任务得不到响应；</p></li><li><p>如果提交的任务依赖于其他任务，那么除非线程池无限大，否则可能造成死锁。</p></li><li><p>将可以并行进行的方法（任务）放入线程的run（或call）方法里执行，然后把线程放入线程池就可以实现线程池调度任务了。</p></li></ul><h4 id="线程池的应用范围"><a href="#线程池的应用范围" class="headerlink" title="线程池的应用范围"></a>线程池的应用范围</h4><ul><li><p>需要大量的线程来完成任务，且完成任务的时间比较短。 像WEB服务器完成网页请求这样的任务，使用线程池技术是非常合适的。因为单个任务小，而任务数量巨大，你可以想象一个热门网站的点击次数。 但对于长时间的任务，比如一个Telnet连接请求，线程池的优点就不明显了。因为Telnet会话时间比线程的创建时间大多了。</p></li><li><p>对性能要求苛刻的应用，比如要求服务器迅速响应客户请求。</p></li><li><p>接受突发性的大量请求，但不至于使服务器因此产生大量线程的应用。突发性大量客户请求，在没有线程池情况下，将产生大量线程，虽然理论上大部分操作系统线程数目最大值不是问题，短时间内产生大量线程可能使内存到达极限，并出现<code>OutOfMemory</code>的错误。</p></li></ul><h4 id="线程池的合理配置"><a href="#线程池的合理配置" class="headerlink" title="线程池的合理配置"></a>线程池的合理配置</h4><ul><li><p>要想合理地配置线程池，就必须首先分析任务特性，可以从以下几个角度来分析：</p><ul><li><p>任务的性质：CPU密集型任务、IO密集型任务和混合型任务。</p></li><li><p>任务的优先级：高、中和低。</p></li><li><p>任务的执行时间：长、中和短。</p></li><li><p>任务的依赖性：是否依赖其他系统资源，如数据库连接。</p></li></ul></li><li><p>分析</p><ul><li><p>性质不同的任务可以用不同规模的线程池分开处理。CPU密集型任务应配置尽可能小的线程，如配置 <code>Ncpu + 1</code> 个线程的线程池。由于IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如<code>2 * Ncpu</code>。混合型的任务，如果可以拆分，将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐量将高于串行执行的吞吐量。如果这两个任务执行时间相差太大，则没必要进行分解。</p></li><li><p>优先级不同的任务可以使用优先级队列来处理。它可以让优先级高的任务先执行。</p><ul><li>注意：如果一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行。</li></ul></li><li><p>执行时间不同的任务可以交给不同规模的线程池来处理，或者可以使用优先级队列，让执行时间短的任务先执行。</p></li><li><p>依赖数据库连接池的任务，因为线程提交SQL后需要等待数据库返回结果，等待的时间越长，则CPU空闲时间就越长，那么线程数应该设置得越大，这样才能更好地利用CPU。</p></li></ul></li></ul><h4 id="SOURCE"><a href="#SOURCE" class="headerlink" title="SOURCE"></a>SOURCE</h4><ul><li><p><a href="http://www.cnblogs.com/lizhenghn/p/3740186.html" target="_blank" rel="noopener">C++实现的线程池收集</a></p></li><li><p><a href="https://yosef-gao.github.io/2016/04/02/linux-c-threadpool/" target="_blank" rel="noopener">简单的C线程池的实现</a></p></li><li><p><a href="https://liuzho.github.io/2017/04/17/%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%8C%E8%BF%99%E4%B8%80%E7%AF%87%E6%88%96%E8%AE%B8%E5%B0%B1%E5%A4%9F%E4%BA%86/" target="_blank" rel="noopener">JAVA版本的详细解释</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;池&quot;&gt;&lt;a href=&quot;#池&quot; class=&quot;headerlink&quot; title=&quot;池&quot;&gt;&lt;/a&gt;池&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;由于服务器的硬件资源“充裕”，那么提高服务器性能的一个很直接的方法就是以空间换时间，即“浪费”服务器的硬件资源，以换取其运行效率。这就是池的概念。池是一组资源的集合，这组资源在服务器启动之初就完全被创建并初始化，这称为静态资源分配。当服务器进入正是运行阶段，即开始处理客户请求的时候，如果它需要相关的资源，就可以直接从池中获取，无需动态分配。很显然，直接从池中取得所需资源比动态分配资源的速度要快得多，因为分配系统资源的系统调用都是很耗时的。当服务器处理完一个客户连接后，可以把相关的资源放回池中，无需执行系统调用来释放资源。从最终效果来看，池相当于服务器管理系统资源的应用设施，它避免了服务器对内核的频繁访问。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;池可以分为多种，常见的有内存池、进程池、线程池和连接池。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="网络编程" scheme="http://roux.top/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="线程" scheme="http://roux.top/tags/%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>前缀、中缀、后缀表达式转换详解</title>
    <link href="http://roux.top/2017/12/12/%E5%89%8D%E7%BC%80-%E5%90%8E%E7%BC%80-%E4%B8%AD%E7%BC%80/"/>
    <id>http://roux.top/2017/12/12/前缀-后缀-中缀/</id>
    <published>2017-12-11T16:00:00.000Z</published>
    <updated>2017-12-12T06:02:16.905Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><ul><li>前面学过树的遍历及转换，感触不是很深，今天回头来看，另有感悟。这种感悟也是得益于大神的博文，记录于此。</li></ul><a id="more"></a><h4 id="中缀表达式转后缀表达式"><a href="#中缀表达式转后缀表达式" class="headerlink" title="中缀表达式转后缀表达式"></a>中缀表达式转后缀表达式</h4><ul><li><p>假定有中缀表达式 <code>1 + (( 2 + 3) * 4 ) – 5</code>，请将它转化为后缀表达式。</p><ul><li><p>方法一：利用表达式树</p><ul><li><p>首先将中缀表达式转换为表达式树，然后后序遍历表达式树，所得结果就是后缀表达式。</p></li><li><p>将中缀表达式转化为表达式树方法：表达式树的树叶是操作数，而其他的节点为操作符，根节点为优先级最低且靠右的操作符（如上述表达式优先级最低的是 <code>-</code> 和 <code>+</code>，但 <code>+</code> 更靠右，所以根为 <code>+</code> ），圆括号不包括(后面左右孩子可以类似的递归处理)。如上述中缀表达式转换后的表达式树如下：</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%90%8E/%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%A0%91.jpg" alt="表达式树"></p></li><li><p>经过后序遍历表达式树后得到的后缀表达式为：<code>12 3 + 4 * + 5 –</code></p></li></ul></li><li><p>方法二：利用辅助栈</p><ul><li><p>从左到右遍历中缀表达式的每个操作数和操作符。当读到操作数时，立即把它输出，即成为后缀表达式的一部分；若读到操作符，判断该符号与栈顶符号的优先级，若该符号优先级高于栈顶元素，则将该操作符入栈，否则就一次把栈中运算符弹出并加到后缀表达式尾端，直到遇到优先级低于该操作符的栈元素，然后把该操作符压入栈中。如果遇到 <code>(</code>，直接压入栈中，如果遇到一个 <code>)</code>，那么就将栈元素弹出并加到后缀表达式尾端，但左右括号并不输出。最后，如果读到中缀表达式的尾端，将栈元素依次完全弹出并加到后缀表达式尾端。</p></li><li><p>仍然以上面的表达式为例，其转换过程如下：</p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%90%8E/%E8%BE%85%E5%8A%A9%E6%A0%88.png" alt="辅助栈"></p></li><li><p>利用辅助栈后缀表达式与用表达式树的结果一样，都为：<code>1 2 3 + 4 * + 5 –</code></p></li></ul></li></ul></li></ul><h4 id="后缀表达式转换为中缀表达式"><a href="#后缀表达式转换为中缀表达式" class="headerlink" title="后缀表达式转换为中缀表达式"></a>后缀表达式转换为中缀表达式</h4><ul><li><p>假定有后缀表达式<code>1 2 3 + 4 * +5 –</code>，请将它转化为中缀表达式。</p><ul><li><p>方法一：利用表达式树</p><ul><li><p>从左到右扫面后缀表达式，一次一个符号读入表达式。如果符号是操作数，那么就建立一个单节点树并将它推入栈中。如果符号是操作符，那么就从栈中弹出两个树T1和T2（T1先弹出）并形成一颗新的树，该树的根就是操作符，它的左、右儿子分别是T2和T1。然后将指向这棵新树的指针压入栈中。</p></li><li><p>前三个符号是操作数，因此创建三颗单节点树并将指向它们的指针压入栈中。</p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%90%8E%E8%BD%AC%E4%B8%AD/1.jpg" alt="1"></p></li><li><p><code>+</code>被读入，因此指向最后两颗树的指针被弹出，形成一颗新树，并将指向新树的指针压入栈中。以下的流程图以相同原理执行。</p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%90%8E%E8%BD%AC%E4%B8%AD/2.jpg" alt="2"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%90%8E%E8%BD%AC%E4%B8%AD/3.jpg" alt="3"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%90%8E%E8%BD%AC%E4%B8%AD/4.png" alt="4"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%90%8E%E8%BD%AC%E4%B8%AD/5.jpg" alt="5"></p></li><li><p>最后再中序遍历所得的表达式树即得到我们所需的中缀表达式：<code>1+（（2 + 3）* 4）-5</code></p></li></ul></li></ul></li></ul><h4 id="中缀表达式转换为前缀表达式"><a href="#中缀表达式转换为前缀表达式" class="headerlink" title="中缀表达式转换为前缀表达式"></a>中缀表达式转换为前缀表达式</h4><ul><li><p>假定有中缀表达式<code>1 + (( 2 + 3) * 4 ) – 5</code>，请将它转化为前缀表达式。</p><ul><li><p>方法一：利用表达式树</p><ul><li><p>先将表达式用表达式树来表示，然后再前序遍历表达式树即得到我们所需的前缀表大式。(表达式树前面已经介绍过，这里不再累赘)。</p></li><li><p>此处，经过前序遍历所得前缀表达式为：<code>- + 1 * + 2 3 4 5</code></p></li></ul></li><li><p>方法二：利用辅助栈</p><ul><li><p>首先构造一个运算符栈，然后<strong>从右至左扫描中缀表达式</strong>。如果是操作数，则直接输出，作为前缀表达式的一个直接转换表达式Temp（最后，前缀表达式由该表达式翻转得到）；如果是运算符，则比较优先级：若该运算符优先级大于等于栈顶元素，则将该运算符入栈，否则栈内元素出栈并加到<code>Temp</code>表达式尾端，直到该运算符大于等于栈顶元素的优先级时，再将该运算符压入栈中。遇到右括号直接压入栈中，如果遇到一个左括号，那么就将栈元素弹出并加到<code>Temp</code>表达式尾端，但左右括号并不输出。最后，若运算符栈中还有元素，则将元素一次弹出并加到<code>Temp</code>表达式尾端，最后一步是将<code>Temp</code>表达式翻转。其过程如下图所示：</p></li><li><p>从右到左开始扫描，5为数字放入Temp中，-为操作符入栈。</p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/1.jpg" alt="1"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/2-1.jpg" alt="2"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/2-2.jpg" alt="3"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/3.jpg" alt="4"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/3-2.jpg" alt="5"></p></li><li><p>遇到左括号，元素弹出直到遇到右括号为止。</p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/4.jpg" alt="6"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/5.jpg" alt="7"></p><p>  <img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E4%B8%AD%E8%BD%AC%E5%89%8D/6.jpg" alt="8"></p></li><li><p>所得前缀表达式为：- + 1 * + 2 3 4 5</p></li></ul></li></ul></li></ul><h4 id="前缀表达式转换为中缀表达式："><a href="#前缀表达式转换为中缀表达式：" class="headerlink" title="前缀表达式转换为中缀表达式："></a>前缀表达式转换为中缀表达式：</h4><ul><li><p>假定有前缀表达式 <code>- + 1 * + 2 3 4 5</code>，请将它转化为中缀表达式。</p><ul><li><p>方法一：辅助栈</p><ul><li><p>首先创建一个数字栈。<strong>从右到左扫描前缀表达式</strong>，如果遇到操作数，则入栈。如果遇到操作符，则将栈顶元素弹出（后扫面的数字位于表达式前面），并和操作符结合写成表达式，作为中缀表达式。如果遇到的操作符优先级大于已存在表达式的最后执行操作符的优先级，则将已存在的表达式加上()。如下是前缀表达式转为中缀表达式的示意图：</p><ul><li><p>扫描到操作数直接入栈。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%89%8D%E8%BD%AC%E4%B8%AD/1.jpg" alt="1"></p></li><li><p>扫描到操作符，将两个栈顶元素弹出，并和操作符结合写成表达式。</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%89%8D%E8%BD%AC%E4%B8%AD/2.jpg" alt="2"></p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%89%8D%E8%BD%AC%E4%B8%AD/3.jpg" alt="3"></p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%89%8D%E8%BD%AC%E4%B8%AD/4.jpg" alt="4"></p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%89%8D%E8%BD%AC%E4%B8%AD/5.jpg" alt="5"></p></li><li><p>表达式不是<code>（2+3）*4 + 1</code>，因为 1 比 2、3、4 后扫描到。(如果不好理解，可以这样思考：遇到操作符出栈结合成为一个表达式，然后将这个表达式入栈，然后继续下一步(将整个表达式当做一个整体操作数))</p><p><img src="https://raw.githubusercontent.com/paulRoux/Pictures/master/%E6%A0%91/%E5%89%8D%E8%BD%AC%E4%B8%AD/6.jpg" alt="6"></p></li><li><p>表达式不是<code>5-（1+（2+3）*4）</code>，因为 5 是最早扫面到的数字。</p></li></ul></li><li><p>所以中缀表达式为<code>(1 +（2 + 3）* 4） - 5</code>。</p></li></ul></li></ul></li></ul><h4 id="SOURCE"><a href="#SOURCE" class="headerlink" title="SOURCE"></a>SOURCE</h4><ul><li><a href="http://blog.csdn.net/walkerkalr/article/details/22798365" target="_blank" rel="noopener">http://blog.csdn.net/walkerkalr/article/details/22798365</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;前面学过树的遍历及转换，感触不是很深，今天回头来看，另有感悟。这种感悟也是得益于大神的博文，记录于此。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="数据结构" scheme="http://roux.top/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="树" scheme="http://roux.top/tags/%E6%A0%91/"/>
    
  </entry>
  
</feed>
